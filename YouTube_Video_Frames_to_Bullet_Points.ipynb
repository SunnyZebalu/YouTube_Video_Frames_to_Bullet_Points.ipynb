{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract\n",
        "!pip install pytube\n",
        "# Install Tesseract OCR in Colab\n",
        "!apt-get install tesseract-ocr\n",
        "!apt-get install libtesseract-dev\n",
        "# Install yt-dlp\n",
        "!pip install yt-dlp"
      ],
      "metadata": {
        "id": "xTbApfLn2tCk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e02dc61c-a35c-4f8a-c0fe-da193d5380e8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n",
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m984.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 2s (3,178 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 121753 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libarchive-dev libleptonica-dev\n",
            "The following NEW packages will be installed:\n",
            "  libarchive-dev libleptonica-dev libtesseract-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 3,743 kB of archives.\n",
            "After this operation, 16.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libarchive-dev amd64 3.6.0-1ubuntu1 [581 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libleptonica-dev amd64 1.82.0-3build1 [1,562 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libtesseract-dev amd64 4.1.1-2.1build1 [1,600 kB]\n",
            "Fetched 3,743 kB in 1s (2,757 kB/s)\n",
            "Selecting previously unselected package libarchive-dev:amd64.\n",
            "(Reading database ... 121800 files and directories currently installed.)\n",
            "Preparing to unpack .../libarchive-dev_3.6.0-1ubuntu1_amd64.deb ...\n",
            "Unpacking libarchive-dev:amd64 (3.6.0-1ubuntu1) ...\n",
            "Selecting previously unselected package libleptonica-dev.\n",
            "Preparing to unpack .../libleptonica-dev_1.82.0-3build1_amd64.deb ...\n",
            "Unpacking libleptonica-dev (1.82.0-3build1) ...\n",
            "Selecting previously unselected package libtesseract-dev:amd64.\n",
            "Preparing to unpack .../libtesseract-dev_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Setting up libleptonica-dev (1.82.0-3build1) ...\n",
            "Setting up libarchive-dev:amd64 (3.6.0-1ubuntu1) ...\n",
            "Setting up libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2024.3.10-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting brotli (from yt-dlp)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2024.2.2)\n",
            "Collecting mutagen (from yt-dlp)\n",
            "  Downloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodomex (from yt-dlp)\n",
            "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.17 in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2.0.7)\n",
            "Collecting websockets>=12.0 (from yt-dlp)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.31.0->yt-dlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.31.0->yt-dlp) (3.6)\n",
            "Installing collected packages: brotli, websockets, pycryptodomex, mutagen, yt-dlp\n",
            "Successfully installed brotli-1.1.0 mutagen-1.47.0 pycryptodomex-3.20.0 websockets-12.0 yt-dlp-2024.3.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mCdexvJjg5dD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c638b0-8899-4bd8-e7c5-238ee2e9990f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.14.2-py3-none-any.whl (262 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/262.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/262.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/262.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.4/262.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.14.2\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "def get_completion(prompt, model=\"gpt-4-0125-preview\"): # Andrew mentioned that the prompt/ completion paradigm is preferable for this class\n",
        "    client = OpenAI(\n",
        "        # This is the default and can be omitted\n",
        "        api_key=userdata.get('OPENAI_API_KEY'),\n",
        "    )\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = client.chat.completions.create(\n",
        "        messages=messages,\n",
        "        model=model,\n",
        "        temperature=0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "from pytube import YouTube\n",
        "from PIL import Image\n",
        "from difflib import SequenceMatcher\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove extra spaces\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    # Remove blank lines\n",
        "    lines = text.split('\\n')\n",
        "    non_blank_lines = [line for line in lines if line.strip()]\n",
        "    cleaned_text = '\\n'.join(non_blank_lines)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "def similarity(a, b):\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "\n",
        "# def download_yt_video(video_url):\n",
        "#   yt = YouTube(video_url)\n",
        "#   video = yt.streams.filter(file_extension='mp4').first()\n",
        "#   video.download('.')\n",
        "\n",
        "#   # Update Tesseract OCR path for Colab\n",
        "#   pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
        "\n",
        "#   # Path to the downloaded video file\n",
        "#   video_path = './' + video.default_filename\n",
        "#   return video_path\n",
        "\n",
        "def download_yt_video(video_url):\n",
        "    # Path to the downloaded video file\n",
        "    video_path = './' + video_url[-11:] + '.mp4'\n",
        "    print(video_path)\n",
        "\n",
        "    # Use yt-dlp to download the best quality mp4 video\n",
        "    command = f'yt-dlp -f \"best[ext=mp4]\" \"{video_url}\" -o \"{video_path}\"'\n",
        "    subprocess.run(command, shell=True, check=True)\n",
        "\n",
        "    return video_path\n",
        "\n",
        "\n",
        "def run_ocr_on_video(video_path, read_rate=250):\n",
        "  # Open the video file\n",
        "  video = cv2.VideoCapture(video_path)\n",
        "\n",
        "  # Initialize an empty string to store the extracted text\n",
        "  extracted_text = []\n",
        "\n",
        "  # Loop through the frames of the video\n",
        "  counter = 0\n",
        "  previous_text = \"*****\"\n",
        "  while video.isOpened():\n",
        "    counter += 1\n",
        "    # Read every \"read_rate\" frame\n",
        "    for i in range(read_rate):\n",
        "      ret, frame = video.read()\n",
        "\n",
        "    # Check if frame reading was successful\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert the frame to PIL Image format\n",
        "    image = Image.fromarray(frame)\n",
        "\n",
        "    # Perform OCR on the image\n",
        "    text = pytesseract.image_to_string(image)\n",
        "    text = clean_text(text)\n",
        "    if text and text != previous_text:\n",
        "      if text.find(previous_text) < 0 and similarity(text, previous_text) < 0.5:\n",
        "        extracted_text += [text]\n",
        "      else:\n",
        "        extracted_text[-1] = text\n",
        "      print(counter, \": \", len(extracted_text), \": \", text, \"\")\n",
        "      previous_text = text\n",
        "\n",
        "  # Release the video capture\n",
        "  video.release()\n",
        "  return extracted_text\n"
      ],
      "metadata": {
        "id": "YO5SdcDIylgO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yt_video_to_text(video_url):\n",
        "  video_path = download_yt_video(video_url)\n",
        "  extracted_text = run_ocr_on_video(video_path)\n",
        "  return extracted_text\n",
        "\n",
        "\n",
        "def text_to_bullets(extracted_text, prompt_body):\n",
        "  prompt = f\"\"\"{prompt_body} ```{\"| \".join(extracted_text)}```\n",
        "  \"\"\"\n",
        "  print(prompt)\n",
        "  response = get_completion(prompt)\n",
        "  return response\n",
        "\n",
        "\n",
        "def yt_video_to_bullet(video_url, prompt_body):\n",
        "  extracted_text = yt_video_to_text(video_url)\n",
        "\n",
        "  response = text_to_bullets(extracted_text, prompt_body)\n",
        "  return response\n"
      ],
      "metadata": {
        "id": "sAf4SoeZ74qd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YouTube video URL\n",
        "\n",
        "video_url = 'https://www.youtube.com/watch?v=hDRDx5HtiBs'\n",
        "prompt_body = \"\"\"\n",
        "The text is extracted from slides images and each slide is\n",
        " separated by a |. Your task is to\n",
        " extract all the main points in the text in organized bullet and\n",
        " sub-bullet points. Don't miss any points in the text, don't change order of the text, don't change the words.\n",
        "\n",
        "Text:\n",
        "\"\"\"\n",
        "\n",
        "response = yt_video_to_bullet(video_url, prompt_body)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "Y-6d_sxjhj1-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c4be450-3879-42d7-cd9d-9469a3b08e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./hDRDx5HtiBs.mp4\n",
            "6 :  1 :  i J Pr , AL ls \n",
            "7 :  2 :  Sq = Caroline Uhler Director, Eric & Wendy Schmidt Center \n",
            "8 :  2 :  Caroline Uhler Director, Eric & Wendy Schmidt Center <4 re \n",
            "9 :  2 :  P< Caroline Uhler S Director, Eric & Wendy Schmidt Center \n",
            "11 :  3 :  . \n",
            "17 :  4 :  ERIC AND WENDY TEI Guiding Diffusion Models Towards Generative Optimization Mengdi Wang Princeton Al2 Initiative Electrical and Computer Engineering, Center for Statistics & Machine Learning Princeton University \n",
            "18 :  4 :  sien ERIC AND WENDY CHMIDT CENTER AT BROAD INSTITUTE Guiding Diffusion Models Towards Generative Optimization Mengdi Wang Princeton Al2 Initiative Electrical and Computer Engineering, Center for Statistics & Machine Learning Princeton University \n",
            "21 :  4 :  ERIC AND WENDY SCHMIDT CENTER Guiding Diffusion Models Towards 3ROAD Generative Optimization Ley Mengdi Wang Princeton Al2 Initiative Electrical and Computer Engineering, Center for Statistics & Machine Learning Princeton University \n",
            "22 :  5 :  ERIC AND WENDY SCHMIDT CENTER 1998 2012 Deep Generative Al 2015 2018 2021 = 2022 2023 = 2024 ~ Thanks to blogs by Rockwell Anyoha , Toloka Team and Rick Merritt A \n",
            "23 :  5 :  ERIC AND WENDY SCHMIDT CENTER Emergence of Generative Al = Po] vido Ltr ng ChatGPT BERT Stable Diffusion Transformer Deep Generative Al 1998 2012 2015-2018 2021 = 2022 2023 = 2024 ~ Thanks to blogs by Rockwell Anyoha , Toloka Team and Rick Merritt A \n",
            "24 :  5 :  ERIC AND WENDY SCHMIDT CENTER 1998 2012 ChatGPT Stable Diffusion Deep Generative Al 2015 2018 2021 2022 2023 2024 ~- Thanks to blogs by Rockwell Anyoha , Toloka Team and Rick Merritt A \n",
            "25 :  5 :  ERIC AND WENDY SCHMIDT CENTER 1998 2012 Transformer eee Deep Generative Al 2015 2018 2021 2022 2023 2024 ~- Thanks to blogs by Rockwell Anyoha , Toloka Team and Rick Merritt A \n",
            "26 :  5 :  ERIC AND WENDY SCHMIDT CENTER Emergence of Generative Al Sy tee tr DALL-E 3 ChatGPT BERT Stable Diffusion Transformer STITU 4 Deep Generative Al 1998 2012 2015 2018 2021 = 2022 2023 = 2024 ~ Thanks to blogs by Rockwell Anyoha , Toloka Team and Rick Merritt \n",
            "27 :  5 :  ERIC AND WENDY SCHMIDT CENTER Emergence of Generative Al ChatGPT Stable Diffusion Deep Generative Al 1998 2012 2015 2018 2021 = 2022 2023 = 2024 ~ Thanks to blogs by Rockwell Anyoha , Toloka Team and Rick Merritt \n",
            "28 :  5 :  ERIC AND WENDY SCHMIDT CENTER Emergence of Generative Al eT fo ih Meecbib ad 1998 2012 LLaMa2 DALL-E 3 ChatGPT BERT Stable Diffusion Transformer Deep Generative Al 2015 2018 2021 2022 2023 2024 ~- Thanks to blogs by Rockwell Anyoha , Toloka Team and Rick Merritt A \n",
            "29 :  5 :  ERIC AND WENDY SCHMIDT CENTER 1998 = 2012 LLaMa2 DALL-E 3 ChatGPT BERT Stable Diffusion Transformer Deep Generative Al 2015 2018 2021 2022 2023 2024 ~- Thanks to blogs by Rockwell Anyoha , Toloka Team and Rick Merritt \n",
            "30 :  5 :  ERIC AND WENDY SCHMIDT CENTER 1998 2012 LLaMa2 DALL-E 3 ChatGPT Stable Diffusion Transformer Deep Generative Al 2015 2018 2021 = 2022 2023 = 2024 ~ Thanks to blogs by Rockwell Anyoha , Toloka Team and Rick Merritt A \n",
            "31 :  5 :  ERIC AND WENDY SCHMIDT CENTER Emergence of Generative Al oy Cy dial ChatGPT Stable Diffusion Deep Generative Al 1998 2012 2015 2018 2021 = 2022 2023 2024 ~- Thanks to blogs by Rockwell Anyoha , Toloka Team and Rick Merritt \n",
            "32 :  5 :  ERIC AND WENDY SCHMIDT CENTER Emergence of Generative Al 29 Coie tr ChatGPT BERT Stable Diffusion Transformer Deep Generative Al 1998 2012 2015 2018 2021 = 2022 2023 2024 ~- Thanks to blogs by Rockwell Anyoha , Toloka Team and Rick Merritt \n",
            "33 :  6 :  ERIC AND WENDY iq TER Family of Deep Generative Al VAE (Kingma & Welling, 2013) \n",
            "34 :  6 :  Family of Deep Generative Al VAE (Kingma & Welling, 2013) \n",
            "35 :  6 :  ste ERIC AND WENDY CHMIDT CENTER Family of Deep Generative Al VAE (Kingma & Welling, 2013) \n",
            "36 :  6 :  ERIC AND WENDY R SCHMIDT CENTE N Family of Deep Generative Al 49p039q E BROAD INSTITUTE VAE (Kingma & Welling, 2013) \n",
            "37 :  6 :  ERIC AND WENDY CHMIDT CENTER 49p099q VAE (Kingma & Welling, 2013) \n",
            "38 :  6 :  stig ERIC AND WENDY CHMIDT CENTER 49p039q VAE (Kingma & Welling, 2013) \n",
            "39 :  6 :  Family of Deep Generative Al VAE (Kingma & Welling, 2013) \n",
            "40 :  6 :  Family of Deep Generative Al Real Real dG A, munis ff Discriminator, Fake —{] Fake Generator VAE (Kingma & Welling, 2013) GAN (Goodfellow et al., 2014) \n",
            "41 :  6 :  ERIC AND WENDY TER Family of Deep Generative Al (—sf ™ VAE (Kingma & Welling, 2013) GAN (Goodfellow et al., 2014) \n",
            "42 :  6 :  ERIC AND WENDY = SCHMIDT CENTER Family of Deep Generative Al (—af ™ VAE (Kingma & Welling, 2013) GAN (Goodfellow et al., 2014) \n",
            "43 :  6 :  ERIC AND WENDY = SCHMIDT CENTER Family of Deep Generative Al ~ VAE (Kingma & Welling, 2013) GAN (Goodfellow et al,, 2014) \n",
            "44 :  6 :  ERIC AND WENDY MIDT CENTER Family of Deep Generative Al (—sf - VAE (Kingma & Welling, 2013) GAN (Goodfellow et al., 2014) \n",
            "45 :  6 :  ERIC AND WENDY SCHMIDT CENTER Family of Deep Generative Al VAE (Kingma & Welling, 2013) GAN (Goodfellow et al., 2014) (ot) Train Generative Al Output ——p —e @ “ \n",
            "46 :  6 :  ERIC AND WENDY SCHMIDT CENTER Family of Deep Generative Al Fake ) GAN (Goodfellow et al., 2014) VAE (Kingma & Welling, 2013 Generative Al Output —p \n",
            "47 :  6 :  ERIC AND WENDY TEI Z VAE (Kingma & Welling, 2013) GAN (Goodfellow et al., 2014) Generative Al Output = pats | ——— \n",
            "48 :  6 :  ERIC AND WENDY SCHMIDT CENTER Family of Deep Generative Al f Fake: VAE (Kingma & Welling, 2013) GAN (Goodfellow et al., 2014) Generative Al \n",
            "49 :  6 :  ERIC AND WENDY SCHMIDT CENTER Family of Deep Generative Al 7 Fake: VAE (Kingma & Welling, 2013) GAN (Goodfellow et al., 2014) Generative Al omer on - gS \n",
            "50 :  7 :  ERIC AND WENDY 2), SCHMIDT CENTER Diffusion model \n",
            "51 :  7 :  ERIC AND WENDY SCHMIDT CENTER Diffusion model \n",
            "53 :  7 :  ERIC AND WENDY CHMIDT CENTER Diffusion model Sora by OpenAl RFDiffusion by UW \n",
            "54 :  7 :  ERIC AND WENDY SCHMIDT CENTER Diffusion model Sora by OpenAl RFDiffusion by UW \n",
            "55 :  7 :  ERIC AND WENDY CHMIDT CENTER Diffusion model Sora by OpenAl RFDiffusion by UW \n",
            "57 :  7 :  ERIC AND WENDY CHMIDT CENTER Diffusion model RFDiffusion by UW \n",
            "58 :  7 :  ERIC AND WENDY SCHMIDT CENTER Diffusion model Sora by OpenAl RFDiffusion by UW \n",
            "59 :  8 :  A Revolution - Diffusion Model * Sequential transformation Noise Data =—z Backbone of Stable Diffusion, DALL-E, Sora etc. (Sohl-Dickstein et al., 2015) (Song and Ermon, 2019) (Ho et al., 2020) \n",
            "60 :  8 :  ERIC AND WENDY CHMIDT CENTER A Revolution - Diffusion Model * Sequential transformation Noise Data Backbone of Stable Diffusion, DALL-E, Sora etc. (Sohl-Dickstein et al., 2015) (Song and Ermon, 2019) (Ho et al., 2020) \n",
            "61 :  8 :  scm ERIC AND WENDY CHMIDT CENTER A Revolution - Diffusion Model * Sequential transformation Noise Data a eet Backbone of Stable Diffusion, DALL-E, Sora etc. (Sohl-Dickstein et al., 2015) (Song and Ermon, 2019) (Ho et al., 2020) \n",
            "62 :  8 :  cn ERIC AND WENDY CHMIDT CENTER A Revolution - Diffusion Model * Sequential transformation Noise Data eS Backbone of Stable Diffusion, DALL-E, Sora etc. (Sohl-Dickstein et al., 2015) (Song and Ermon, 2019) (Ho et al., 2020) \n",
            "63 :  8 :  ERIC AND WENDY CHMIDT CENTER A Revolution - Diffusion Model * Sequential transformation Noise ae Backbone of Stable Diffusion, DALL-E, Sora etc. (Sohl-Dickstein et al., 2015) (Song and Ermon, 2019) (Ho et al., 2020) \n",
            "64 :  8 :  A Revolution - Diffusion Model * Sequential transformation Noise Data = —e (Sohl-Dickstein et al., 2015) > 890M parameters (Song and Ermon, 2019) (Stable Diffusion) (Ho et al., 2020) \n",
            "65 :  8 :  ERIC AND WENDY SCHMIDT CENTER A Revolution - Diffusion Model * Sequential transformation Noise i ie liao (Sohl-Dickstein et al., 2015) > 890M parameters (Song and Ermon, 2019) (Stable Diffusion) (Ho et al., 2020) \n",
            "66 :  8 :  ERIC AND WENDY CHMIDT CENTER SCHMIDT CENTER AY HROAD INSTITUTE A Revolution - Diffusion Model * Sequential transformation Noise Data - saa Backbone of Stable Diffusion, DALL-E, Sora etc. aI (Sohl-Dickstein et al., 2015) > 890M parameters (Song and Ermon, 2019) (Stable Diffusion) (Ho et al., 2020) \n",
            "67 :  8 :  stig ERIC AND WENDY CHMIDT CENTER A Revolution - Diffusion Model * Sequential transformation Noise Data (Sohl-Dickstein et al., 2015) > 890M parameters (Song and Ermon, 2019) (Stable Diffusion) (Ho et al., 2020) \n",
            "68 :  9 :  ERIC AND WENDY CHMIDT CENTER US an exoan instirure = SCHMIDT CI AT nRoap insti A Revolution - Diffusion Model * Sequential transformation Noise Data ee ee a sgam__ Backbone of Stable Diffusion, DALL-E, Sora etc. ra | 1 2 ee iH ~ ie iH (Sohl-Dickstein et al., 2015) > 890M parameters (Song and Ermon, 2019) (Stable Diffusion) (Ho et al., 2020) \n",
            "69 :  9 :  A Revolution - Diffusion Model * Sequential transformation - sam Backbone of Stable Diffusion, DALL-E, Sora etc. i . | +H 1 ' He] —— HA san bit — ie (Sohl-Dickstein et al., 2015) > 890M parameters (Song and Ermon, 2019) (Stable Diffusion) (Ho et al., 2020) \n",
            "70 :  9 :  A Revolution - Diffusion Model * Sequential transformation Noise Data 1+ Sco (Sohl-Dickstein et al., 2015) > 890M parameters (Song and Ermon, 2019) (Stable Diffusion) (Ho et al., 2020) \n",
            "71 :  9 :  ERIC AND WENDY CHMIDT CENTER A Revolution - Diffusion Model * Sequential transformation Noise Data a; e 4 Mt ‘ | tH HH ie ry | sem Backbone of Stable Diffusion, DALL-E, Sora etc. ey ; (Sohl-Dickstein et al., 2015) > 890M parameters (Song and Ermon, 2019) (Stable Diffusion) (Ho et al., 2020) \n",
            "72 :  10 :  ERIC AND WENDY SCHMIDT CENTER Foundations of Diffusion Models \n",
            "75 :  11 :  en ERIC AND WENDY SCHMIDT CENTER Diffusion Model Generates Samples * Generate samples from noise Noise Data 2 nd eS wes [-_i—_-i—® \n",
            "76 :  11 :  ERIC AND WENDY SCHMIDT CENTER ERIC AND WENDY. )) SCHMIDT CENTER je Diffusion Model Generates Samples © sr stone sire * Generate samples from noise Noise -- Credit: online \n",
            "77 :  11 :  ERIC AND WENDY Y SCHMIDT CENTER Y Diffusion Model Generates Samples * Generate samples from noise Noise Data -- Credit: online \n",
            "78 :  11 :  SCHMIDT CENTER ERIC AND WENDY y Diffusion Model Generates Samples * Generate samples from noise Noise Data =—z -- Credit: online \n",
            "79 :  12 :  ERIC AND WENDY SCHMIDT CENTER RIC AND WENDY SCHMIDT CENTER Diffusion for protein structure generation Forward (noising) process ———S— re r ome No. 1) ¥ Single ¥ = step Gaussian ~ Protein < noise co Rte, —> cg x structure se Z e. ee a We Pe LA Xx, xX. X > Reverse (generative) process -- credit: RFDiffusion from Baker's group, 2023 \n",
            "80 :  13 :  ERIC AND WENDY SCHMIDT CENTER Diffusion for protein structure generation ’ . Forward (noising) process —_\\| No 1) ¥ Single ¥ S step Gaussian ~ . < Protein noise see a — 4 000 x structure ™ Bee AR , \\AX X, LA X, > Reverse (generative) process -- credit: RFDiffusion from Baker's group, 2023 \n",
            "81 :  13 :  ERIC AND WENDY SCHMIDT CENTER AND WENDY SCHMIDT CENTER Diffusion for protein structure generation Forward (noising) process —_ Noy —¥ ¥ Kd Ss Single step Gaussian ~ a * « Protein noise R i oe x structure oe ke We T LA Xx, xX, LA X, = Reverse (generative) process -- credit: RFDiffusion from Baker's group, 2023 \n",
            "82 :  13 :  ERIC AND WENDY SCHMIDT CENTER iCHMIDT CENTER Diffusion for protein structure generation Forward (noising) process ——————— No. 1) ¥ Single ¥ step < Protein Rate, —> oe X structure as ee, We x, LA X, Xs A X ne Reverse (generative) process B Gaussian ~ noise -- credit: RFDiffusion from Baker's group, 2023 \n",
            "83 :  13 :  ERIC AND WENDY SCHMIDT CENTER Diffusion for protein structure generation . Forward (noising) process ——E No 1) ¥ Single ¥ step , Protein t ve —> ee ata We structure x, LA x, Xs LA X ns Reverse (generative) process Gaussian ~ noise -- credit: RFDiffusion from Baker's group, 2023 \n",
            "84 :  13 :  ERIC AND WENDY R AND WENDY SCHMIDT CENTER Diffusion for protein structure generation J Forward (noising) process _——— No. ¥ Single ¥ step * als Protein Rex —> 1 0D) ¥ structure ' ek, We x, LA x, xX, AX, — ae Reverse (generative) process Gaussian ~ noise -- credit: RFDiffusion from Baker's group, 2023 \n",
            "85 :  14 :  ERIC AND WENDY SCHMIDT CENTER SCHMIDT CENTER Diffusion for protein structure generation it Forward (noising) process | eee _ No 1) ¥ Single ¥ step A « Protein 6 > oe X structure : ae Se We x, LA Xx, Xs X ee Reverse (generative) process Gaussian ~ noise -- credit: RFDiffusion from Baker's group, 2023 \n",
            "86 :  14 :  ERIC AND WENDY SCHMIDT CENTER ERS ar mgoap institute SCHMIDT CENTER | Diffusion for protein structure generation Forward (noising) process << No 1) ¥ Single ¥ step Protein sn: < WS truct at a — : A, We structure x, LA Xx, Xs X ———— Reverse (generative) process Gaussian ~ noise -- credit: RFDiffusion from Baker's group, 2023 \n",
            "87 :  14 :  ERIC AND WENDY SCHMIDT CENTER Diffusion for protein structure generation Forward (noising) process <—_ ) No. ¥ Single ¥ r step Protein t a — ee AS structure x, LA Xx, Xen LA X en Reverse (generative) process Gaussian ~» % noise -- credit: RFDiffusion from Baker's group, 2023 \n",
            "88 :  15 :  ERIC AND WENDY R SCHMIDT Forward Process - Noise Corruption =. —— * Noise corruption process Add Gaussian noise Data = Approx. Noise M @ o~ “S @ IA. Z '% dX; = ~5 Kut +dW, * The noise corruption Lee ee Data Approx. Noise \n",
            "89 :  15 :  sti ERIC AND WENDY CHMIDT CENTER Forward Process - Noise Corruption * Noise corruption process Add Gaussian noise Data - mA Approx. Noise M. @—®@ @) i i ¢ g 1 dX; = Ta Xedt +dW; * The noise corruption Lee oe Data Approx. Noise \n",
            "90 :  15 :  ERIC AND WENDY i R Forward Process - Noise Corruption + Noise corruption process Add Gaussian noise > Data ener ee Approx. Noise t M @—@ @ IA. é é& 1 dX; = ~aXedt +dW; * The noise corruption Lem ee Data Approx. Noise \n",
            "91 :  15 :  ERIC AND WENDY CHMIDT CENTER Forward Process - Noise Corruption * Noise corruption process Add Gaussian noise Data Approx. Noise 1 dX, = ~_Xedt +dW;, * The noise corruption Lem ee Data Approx. Noise \n",
            "92 :  15 :  Forward Process - Noise Corruption * Noise corruption process Add Gaussian noise Data ert Approx. Noise o-oo 8 MM : aa dx; = —5 Xu + dw, * The noise corruption Data Approx. Noise \" \n",
            "93 :  15 :  Forward Process - Noise Corruption * Noise corruption process Add Gaussian noise 1 dx, = —Xedt = dW, * The noise corruption Data Approx. Noise \"1 \n",
            "96 :  15 :  ERIC AND WENDY R Forward Process - Noise Corruption + Noise corruption process Add Gaussian noise Data Approx. Noise ' M @—@ © é & a 1 dX; = Ta Xedt +dW; * The noise corruption Lebo bb Data Approx. Noise \n",
            "97 :  16 :  ERIC AND WENDY R SCHMIDT CENTER Backward Process - Sample Generation * Time reversal in distribution Noise Vlog pr-(Xi-) Data oo oS Z ¢ The math (Anderson, 1982; Haussmann and Pardoux, 1986) 1 dX, = ~aXedt +dW; 1 Backward dxf = gx + dt+dWw, Score Function Brownian \n",
            "98 :  16 :  ERIC AND WENDY i R Backward Process - Sample Generation * Time reversal in distribution Noise Vlog pr-(Xi\") Data a Z * The math (Anderson, 1982; Haussmann and Pardoux, 1986) 1 dX, = ~aXedt +dW; Backward = Xf = ext HV log pr_1(X¢-) dt+dW, Score Function Brownian \n",
            "99 :  16 :  ERIC AND WENDY R Backward Process - Sample Generation * Time reversal in distribution Noise Vlog pr-(Xi\") Data oo *@-@ Zz * The math (Anderson, 1982; Haussmann and Pardoux, 1986) 1 dX, = ~aXedt +dW;, Backward dxf = ext HV log pr—-1(X, )) dt +dW;, Score Function Brownian \n",
            "100 :  16 :  ERIC AND WENDY R Backward Process - Sample Generation * Time reversal in distribution Noise Vlog pr-(Xi\") Data Semen oie zt * The math (Anderson, 1982; Haussmann and Pardoux, 1986) 1 dX, = ~aXedt +dW; Backward = Xf = ext HV log pr_1(X¢-) dt+ dw, Score Function Brownian \n",
            "102 :  16 :  ERIC AND WENDY R Backward Process - Sample Generation * Time reversal in distribution Noise Vlog pr-(Xi\") Data © © *_ © -® Ze ¢ The math (Anderson, 1982; Haussmann and Pardoux, 1986) 1 dX, = ~Xedt + dW, Backward = dX ext HV log pr_1(X¢-) dt+aW, Brownian Score Function \n",
            "103 :  16 :  ERIC AND WENDY R Backward Process - Sample Generation * Time reversal in distribution Noise Vlog pr-(Xi\") Data epee me Zz ¢ The math (Anderson, 1982; Haussmann and Pardoux, 1986) uy dX, = ~aXedt +dW; Backward ax - (5 XP Vlog pr_i(X)] dt + dW, Score Function Brownian \n",
            "104 :  16 :  Backward Process - Sample Generation * Time reversal in distribution Noise Vlog pr-(Xi-) Data Q—@©—_*— ©@—® a c a ¢ The math (Anderson, 1982; Haussmann and Pardoux, 1986) Teer, 1 Forward dX, = ~gXede +dw, Backward dxf = Face +iV log pr—1(X/)) dt + dW; Score Function Brownian \n",
            "105 :  17 :  ERIC AND WENDY TER Backward Process - Sample Generation * Time reversal in distribution Noise Vlog pr-e(Xi\") Data ©-@ ~“—_@-@ =z 1 = ~aXedt +dW, dX, Backward dxf - (5 xP +HV log pr_ nee dt+dW, Score Function Brownian \n",
            "106 :  17 :  Backward Process - Sample Generation * Time reversal in distribution Noise V log pr-i(X7\") Data ©—@—_*—-©—® a it * The math (Anderson, 1982; Haussmann and Pardoux, 1986) Forward dX; = —5Xat +dW; Backward dxf = [pao iV log pr— a7 dt+dW, tee Score Function Brownian a \n",
            "108 :  18 :  Forward and Backward Coupling Gaussian Noise Data Ssrigcr as, Approx. Noise . an — e Or C) OAD Density, Vi ~ Noise Data Viogpr-t mes \n",
            "109 :  18 :  ERIC AND WENDY CHMIDT CENTER Forward and Backward Coupling Gaussian Noise . ieee ral Approx. Noise Data ae me O—O: O—O Sy eta vi ss Noise Data Viogpr-t mes \n",
            "110 :  18 :  ERIC AND WENDY SCHMIDT CENTER HMIDT CENTER Forward and Backward Coupling _ . Gaussian Noise Spiciceacns Approx. Noise \n",
            "111 :  19 :  Forward and Backward Coupling Gaussian Noise Data eck eae ‘. Approx. Noise A “. Forward Se . ‘¢ ¢ > a ai Ne x! Density 7\" °s, 4 -<ften Noise nS Data | ~ \n",
            "113 :  20 :  Approx. Noise \n",
            "118 :  20 :  san ERIC AND WENDY HMIDT CENTER Forward and Backward Coupling Gaussian Noise . Data ence, Approx. Noise © on Z H.. me OSty 7 Noise Data O—-F——\"o—-8 Viogpr-t mes ¥ \n",
            "119 :  21 :  Forward and Backward Coupling Gaussian Noise Data eae eae So = s Density co Forward © an Data o_o a Approx. Noise \n",
            "120 :  22 :  Score Function Estimation * Conceptual least-square loss Score Network be ff 2 [IV t06p4(e4) — s(ee, 18] at 0 \n",
            "121 :  22 :  stem ERIC AND WENDY CHMIDT CENTER Score Function Estimation * Conceptual least-square loss Score Network T [ Bz, [|IV log pe(xe) — s(x, t)|3] at \n",
            "122 :  22 :  ERIC AND WENDY CHMIDT CENTER Score Function Estimation oll * Conceptual least-square loss aa is Ez, {|| V log pe(ae) — s(ae, t)|I3] dt * Early-stopping loss n / Ez, [|| Vlogpe(«.) — s(x, t)|3] at \n",
            "123 :  22 :  ERIC AND WENDY CHMIDT CENTER Score Function Estimation SSoane cena * Conceptual least-square loss [Bec lV os pe.) — (0,18) * Early-stopping loss a / Ey, [||Vlog pele) — s(t, )|3] dt \n",
            "124 :  22 :  stig ERIC AND WENDY CHMIDT CENTER Score Function Estimation * Conceptual least-square loss [ Ez, {|| V log pe(ae) — s(ae, t)||3] at * Early-stopping loss [Bx [IVosps(ae ~ (2,018) * Equivalent loss (Hyvarinen and Dann, 2005; Vincent 2011) Z)\\— s(ee,¢)|[3] at Added Gaussian Noise \n",
            "129 :  22 :  ERIC AND WENDY CHMIDT CENTER Score Function Estimation * Conceptual least-square loss [Be lV 0s pe.) — (018) a * Early-stopping loss [ Ez, {|| V log pe(ae) — s(ae, t)|I3] at * Equivalent loss (Hyvarinen and Dann, 2005; Vincent 2011) Added Gaussian Noise \n",
            "130 :  23 :  ERIC AND WENDY CHMIDT CENTER Can we trust diffusion models? Forward (noising) process Now) ¥ Single ¥ step Protein Sian <> i oe structure eS “2 We x LAX X, LAX —E Reverse (generative) process Gaussian ~ noise -- credit: RFDiffusion from Baker's group, 2023 \n",
            "131 :  23 :  ERIC AND WENDY CHMIDT CENTER Can we trust diffusion models? Forward (noising) process Now) ¥ Single ¥ step Protein structure Gaussian ~ noise : 5: e NE xX, WAX —————— Reverse (generative) process 1 -- credit: RFDiffusion from Baker's group, 2023 \n",
            "132 :  23 :  ERIC AND WENDY SCHMIDT CENTER ERS at maoap ixsrituTe ERIC AND WENDY SCHMIDT CENTER, Can we trust diffusion models? Forward (noising) process Now) ¥ Single ¥ step Protein 6 NE AE structure X. LAX, ne Reverse (generative) process Gaussian ~ noise -- credit: RFDiffusion from Baker's group, 2023 \n",
            "133 :  23 :  ERIC AND WENDY CHMIDT CENTER Can we trust diffusion models? Forward (noising) process Now) ¥ Single ¥ step Protein : ~~ a structure a “ We x LAX X, LAX = —— Reverse (generative) process Gaussian ~ noise -- credit: RFDiffusion from Baker's group, 2023 \n",
            "134 :  23 :  ERIC AND WENDY SCHMIDT CENTER ERS av mgoap ixstituTe Can we trust diffusion models? Forward (noising) process Now) ¥ Single ¥ step Protein ee AE structure x LAX xX. LA X, ——— Reverse (generative) process Gaussian noise -- credit: RFDiffusion from Baker's group, 2023 | ERIC AND WENDY 1, SCHMIDT CENTER \n",
            "135 :  23 :  ERIC AND WENDY CHMIDT CENTER Can we trust diffusion models? Forward (noising) process Now) ¥ Single ¥ step ESBRO Protein INSTI Rites —> oo structure = a We : x LAX X LAX ———— Reverse (generative) process Gaussian ~ noise -- credit: RFDiffusion from Baker's group, 2023 \n",
            "136 :  23 :  ERIC AND WENDY CHMIDT CENTER Can we trust diffusion models? Forward (noising) process Now) ¥ Single ¥ step Gaussian ~ . Protein noise ai > ir ‘ structure \" ae e We xX LAX X. LAX a Reverse (generative) process Q1: Can diffusion models learn the underlying data distribution? -- credit: RFDiffusion from Baker's group, 2023 \n",
            "137 :  23 :  ERIC AND WENDY SCHMIDT CENTER ERS av mgoav institute Can we trust diffusion models? Forward (noising) process Now) ¥ Single ¥ step Protein ee ES structure x LAX X. LAX, —— Reverse (generative) process Gaussian ~ noise Q1: Can diffusion models learn the underlying data distribution? Q2: Can we control the generation of diffusion models towards specific objectives? How efficiently? -- credit: RFDiffusion from Baker's group, 2023 \n",
            "138 :  23 :  ERIC AND WENDY SCHMIDT CENTER ERIC AND WENDY | SCHMIDT CENTER Can we trust diffusion models? Forward (noising) process Now) ¥ Single ¥ step Protein Se AE structure x LAX xX LAX, — Reverse (generative) process Gaussian ~ noise Q1: Can diffusion models learn the underlying data distribution? Q2: Can we control the generation of diffusion models towards specific objectives? How efficiently? -- credit: RFDiffusion from Baker's group, 2023 \n",
            "139 :  23 :  ERIC AND WENDY CHMIDT CENTER SCHMIDT CENTER Can we trust diffusion models? a Forward (noising) process Now) ¥ Single ¥ step Protein Res, > con structure ee e: We x LAX X. LAX ee Reverse (generative) process Gaussian ~ noise Q1: Can diffusion models learn the underlying data distribution? Q2: Can we control the generation of diffusion models towards specific objectives? How efficiently? -- credit: RFDiffusion from Baker's group, 2023 \n",
            "140 :  23 :  ERIC AND WENDY SCHMIDT CENTER Can we trust diffusion models? Forward (noising) process Now) ¥ Single ¥ step Protein Se ANE structure x LAX xX. LAX, —————— Reverse (generative) process Gaussian ~ noise Q1: Can diffusion models learn the underlying data distribution? Q2: Can we control the generation of diffusion models towards specific objectives? How efficiently? -- credit: RFDiffusion from Baker's group, 2023 \n",
            "141 :  24 :  ERIC AND WENDY SCHMIDT CENTER SCHMIOT CENTER Practical Data Is Structured and Low-Dim [—a m 2 @ iw ea oe 'e Rae Tea pot in 1D \n",
            "142 :  24 :  ERIC AND WENDY SCHMIDT CENTER m2 a a a Ee 7 2 a e Bee Tea pot in 1D \n",
            "143 :  24 :  gies ERIC AND WENDY CHMIDT CENTER ERIC AND WENDY J SCHMIDT CENTER | Practical Data Is Structured and Low-Dim m2 a wg Ba , os a 9 Bean Tea pot in 1D \n",
            "144 :  24 :  ERIC AND WENDY SCHMIDT CENTER Practical Data Is Structured and Low-Dim N | m2 w = fea Tea pot in 1D Sli \n",
            "145 :  24 :  ERIC AND WENDY HMIDT CENTER Practical Data Is Structured and Low-Dim m a wl g Be a e Bean Tea pot in 1D \n",
            "146 :  24 :  ste ERIC AND WENDY CHMIDT CENTER Practical Data Is Structured and Low-Dim m @ ey Sig age Bean Tea pot in 1D \n",
            "147 :  24 :  ERIC AND WENDY SCHMIDT CENTER Practical Data Is Structured and Low-Dim maa yg Sig iq ew Tea pot in 1D \n",
            "148 :  24 :  ic ERIC AND WENDY SCHMIDT CENTER Practical Data Is Structured and Low-Dim tom a3 wee k=5 mut k=10 om k=20 Dimensionality Estimate Tea pot in 1D \n",
            "149 :  24 :  ERIC AND WENDY SCHMIDT CENTER Practical Data Is Structured and Low-Dim tom k=3 wae k=5 mwa k=10 om k=20 0 il a0 @ iw : @ SB. 2 i Tea pot in 1D Effective dim of image datasets -- Figure credit: (Weinberger & Saul, 2006; P. Pope et al., 2021) \n",
            "150 :  24 :  ERIC AND WENDY SCHMIDT CENTER SCHMIDT CENTER Practical Data Is Structured and Low-Dim tom k=3 wae k=5 mwa k=10 ome k=20 0 il £0 @ i] : 2 i A Tea pot in 1D Effective dim of image datasets -- Figure credit: (Weinberger & Saul, 2006; P. Pope et al., 2021) \n",
            "151 :  24 :  ERIC AND WENDY SCHMIDT CENTER Practical Data Is Structured and Low-Dim 0 ga === $40 i a B z TITUTE @ a a i Tea pot in 1D Effective dim of image datasets -- Figure credit: (Weinberger & Saul, 2006; P. Pope et al., 2021) \n",
            "152 :  24 :  ERIC AND WENDY SCHMIDT CENTER ERS av mgoav institute @g ERIC AND WENDY Practical Data Is Structured and Low-Dim Nom k=3 wae k=5 we k=10 om k=20 0 FS a0 @ i] : @ @ . a i Tea pot in 1D Effective dim of image datasets -- Figure credit: (Weinberger & Saul, 2006; P. Pope et al., 2021) \n",
            "153 :  25 :  ERIC AND WENDY R Can Diffusion Model Learn Data's Intrinsic Structure? * The (backward) generation process dt + dW, Brownian Score Function \n",
            "154 :  25 :  ERIC AND WENDY R Can Diffusion Model Learn Data’s Intrinsic Structure? * The (backward) generation process dt + dW, Score Function Brownian \n",
            "155 :  25 :  ERIC AND WENDY R Can Diffusion Model Learn Data's Intrinsic Structure? * The (backward) generation process dt + dW, Score Function Brownian \n",
            "156 :  25 :  Can Diffusion Model Learn Data's Intrinsic Structure? * The (backward) generation process Score Function ” Brownian * Geometric data adds another layer of difficulty Manifold (degenerate) t \n",
            "157 :  25 :  Can Diffusion Model Learn Data's Intrinsic Structure? * The (backward) generation process Score Function - Brownian * Geometric data adds another layer of difficulty Manifold (degenerate) t \n",
            "158 :  25 :  Can Diffusion Model Learn Data's Intrinsic Structure? * The (backward) generation process Score Function” Brownian * Geometric data adds another layer of difficulty Manifold (degenerate) t \n",
            "159 :  26 :  Brownian \n",
            "160 :  26 :  Intrir Brownian \n",
            "161 :  27 :  ERIC AND WENDY CHMIDT CENTER Can Diffusion Model Learn Data's Intrinsic Structure? * The (backward) generation process Score Famtion’ Brownian * Geometric data adds another layer of difficulty Manifold egenerate) \n",
            "162 :  28 :  ERIC AND WENDY SCHMIDT CENTER Let’s work with Low-dimensional Data = w(x) = Az with z~ P,. m A € R*¢ js unknown with orthonormal columns. = z € R¢isa latent variable. = wis a known invertible transformation, e.g., feature map of RKHS and Fourier transform. (For simplicity, we take ~ = J in the sequel.) m= D - ambient dimension; d - intrinsic dimension. \n",
            "163 :  28 :  Let’s work with Low-dimensional Data wl(x) = Az with z~ P,. a A € R?X*4 is unknown with orthonormal columns. = z € R¢isa latent variable. m wis a known invertible transformation, e.g., feature map of RKHS and Fourier transform. (For simplicity, we take 7) = J in the sequel.) a D - ambient dimension; d - intrinsic dimension. \n",
            "164 :  28 :  ERIC AND WENDY SCHMIDT CENTER ERIC AND WENDY SCHMIDT CENT! Let’s work with Low-dimensional Data w\\(x) = Az with z~ P,. = A € R*¢ js unknown with orthonormal columns. = z € R¢ isa latent variable. = wis a known invertible transformation, e.g., feature map of RKHS and Fourier transform. (For simplicity, we take ~ = J in the sequel.) m= D - ambient dimension; d - intrinsic dimension. \n",
            "165 :  28 :  ERIC AND WENDY SCHMIDT CENTER Let’s work with Low-dimensional Data wl(x) = Az with z~ P,. m A € R*¢ js unknown with orthonormal columns. = z € R¢ isa latent variable. = wis a known invertible transformation, e.g., feature map of RKHS and Fourier transform. (For simplicity, we take 7 = J in the sequel.) m= D - ambient dimension; d - intrinsic dimension. \n",
            "166 :  28 :  Let’s work with Low-dimensional Data wl(x) = Az with z~ P,. = A € R?X*4 is unknown with orthonormal columns. = z € R¢isa latent variable. m wis a known invertible transformation, e.g., feature map of RKHS and Fourier transform. (For simplicity, we take 7) = J in the sequel.) a D - ambient dimension; d - intrinsic dimension. \n",
            "167 :  28 :  ERIC AND WENDY SCHMIDT CENTER AMIDT CENTER Let's work with Low-dimensional Data | - ae w(x) =Az with z~ P,. = A € R*¢ js unknown with orthonormal columns. = z € R¢ isa latent variable. = wis a known invertible transformation, e.g., feature map of RKHS and Fourier transform. (For simplicity, we take x = J in the sequel.) mu D - ambient dimension; d - intrinsic dimension. \n",
            "168 :  29 :  Let's also simplify the score network's architecture \n",
            "169 :  29 :  ERIC AND WENDY CHMIDT CENTER Let's also simplify the score network's architecture Shortcut pat Seqnetatan map Kl hose —h-\\(t) A(t) Ré 8 f 8v,0(x, t) 1 t Encoder fo Decoder [eh Hed corsa aw ! { conto La 0 ea tsi bee oo \n",
            "170 :  30 :  hitecture \n",
            "171 :  30 :  rchitecture \n",
            "174 :  30 :  s architecture \n",
            "175 :  30 :  ERIC AND WENDY CHMIDT CENTER Let's also simplify the score network's architecture Shortcut | out segnentaton map h(t) A(t) x RP R! RP 8v,0(x, t) Encoder fo Decoder \n",
            "176 :  31 :  ERIC AND WENDY CHMIDT CENTER Key Insight: Decomposition * Score decomposition V log p(x) = AV log pj (A’ x) — \n",
            "177 :  31 :  stem ERIC AND WENDY CHMIDT CENTER Key Insight: Decomposition NO WENDY MIDT CENTER * Score decomposition V log p(x) = AV log pj (A’ x) — l-e-t \n",
            "178 :  31 :  stim ERIC AND WENDY CHMIDT CENTER Key Insight: Decomposition * Score decomposition On-subspace \n",
            "179 :  31 :  ERIC AND WENDY CHMIDT CENTER Key Insight: Decomposition * Score decomposition On-subspace \n",
            "180 :  31 :  ERIC AND WENDY CHMIDT CENTER Key Insight: Decomposition * Score decomposition V log p:(x) \n",
            "181 :  31 :  cn ERIC AND WENDY CHMIDT CENTER Key Insight: Decomposition * Score decomposition 1 ra \\ {Projection Subspace \n",
            "182 :  31 :  ERIC AND WENDY CHMIDT CENTER Key Insight: Decomposition * Score decomposition \n",
            "183 :  31 :  stim ERIC AND WENDY SCHMIDT CENTER Key Insight: Decomposition * Score decomposition \n",
            "184 :  31 :  cn ERIC AND WENDY CHMIDT CENTER Key Insight: Decomposition * Score decomposition \n",
            "185 :  31 :  gies ERIC AND WENDY CHMIDT CENTER Key Insight: Decomposition * Score decomposition Generated Sample > 7 ihroiection Subspace Lt3@ \n",
            "186 :  31 :  sti ERIC AND WENDY CHMIDT CENTER Key Insight: Decomposition * Score decomposition Generated Sample. \n",
            "187 :  31 :  ERIC AND WENDY HMIDT CENTER Key Insight: Decomposition * Score decomposition Lo @ Generated Sample 7 Nihriection Subspace Lt3@ \n",
            "188 :  31 :  sti ERIC AND WENDY SCHMIDT CENTER Key Insight: Decomposition * Score decomposition Lo @ Generated Sample —\\ 7 hriection Subspace Lt3@ \n",
            "189 :  31 :  IC AND WENDY HMIDT CENTER Key Insight: Decomposition * Score co t Generated 1 Sample Projection Subspace \n",
            "190 :  31 :  Key Insight: Decomposition * Score decomposition V log p:(x) =! FAV log pi(A\"2),— = (Ip = AA’) a! eases ae ; On-subspace Orthogonal Lt, Lty 3 Generated 1 Sample mY 77 NIrlection Subspace Lt \n",
            "191 :  32 :  ERIC AND WENDY CHMIDT CENTER From Subspace to Manifold Diffusion AND WENDY. | SCHMIDT CENTER ey nN * —at = BROF —- ae BROAD Subspace Manifold s, _) oe ts Score = On-subspace + Orthogon Score = On-manifold + Orthog¢ + Curvature-dependent \n",
            "192 :  32 :  Sen ERIC AND WENDY CHMIDT CENTER From Subspace to Manifold Diffusion Subspace Manifold 1 tt, ty Score = On-subspace + Orthogon Score = On-manifold + Orthox + Curvature-dependent ross-term \n",
            "193 :  32 :  ERIC AND WENDY CHMIDT CENTER From Subspace to Manifold Diffusion SCHMIDT CENTER Subspace Manifold TH Le, Lt, y ¢ ° 1 ts Ts Score = On-subspace + Orthogona Score = On-manifold + Orth + Curvature-dependent \n",
            "194 :  32 :  gies ERIC AND WENDY CHMIDT CENTER From Subspace to Manifold Diffusion Subspace Manifold 1 ty Ts s : = Score = On-subspace + Orthogon Score = On-manifold + Orthogor + Curvature-dependent ross-term \n",
            "195 :  32 :  ste ERIC AND WENDY CHMIDT CENTER From Subspace to Manifold Diffusion Subspace Manifold Th Te, Te ty ; ° 1 1 ++ Pee) ~~) Prefection ts Ts Score = On-subspace + logon Score = On-manifold + Orth t + Curvature-dependent ross-term \n",
            "196 :  33 :  gti ERIC AND WENDY CHMIDT CENTER RIC AND WENDY ff SCHMIDT CENTER From Subspace to Manifold Diffusion Subspace Manifold Pe Lt, Lt, y ¢ ° _ | . _ <<) Prafection Local 2 ty projection Score = On-subspace + logon Score = On-manifold + Orthox + Curvature-dependent \n",
            "197 :  34 :  stim ERIC AND WENDY CHMIDT CENTER From Subspace to Manifold Diffusion v SCHMIDT CENTER Subspace Manifold TH te, t% 8 : ; 1 + rae ~~) Prefection . oca * tty projection Score = On-subspace + Orthogon Score = On-manifold + Orthogo + Curvature-dependent ross-term \n",
            "198 :  35 :  Diffusion model is an efficient density estimator Theorem Y (Distribution). Converge to the data distribution at the rate 6 (n- xa ) Y (Fidelity). Deviation to the subspace is bounded by 6 (n\"™*=), -- M. C, K. Huang, T. Zhao, M. Wang. “Score Approximation, Estimation and Distribution Recovery of Diffusion Models on Low-Dimensional Data”, ICML 2023 27 \n",
            "199 :  35 :  ERIC AND WENDY CHMIDT CENTER AS ar proap instirure Diffusion model is an efficient density estimator Theorem Vv (Distribution). Converge to the data distribution at the rate oO (n- xan ) Y (Fidelity). Deviation to the subspace is bounded by 6 (n=), M.C, K. Huang, T. Zhao, M. Wang. \"Score Approximation, Estimation and Distribution Recovery of Diffusion Models on Low-Dimensional Data”, ICML 2023 \n",
            "200 :  35 :  ERIC AND WENDY SCHMIDT CENTER SS at sroap institute Diffusion model is an efficient density estimator Theorem Vv (Distribution). Converge to the data distribution at the rate oO (n- 200 ) Y (Fidelity). Deviation to the subspace is bounded by 6 (\"=\"), M.C, K. Huang, T. Zhao, M. Wang. \"Score Approximation, Estimation and Distribution Recovery of Diffusion Models on Low-Dimensional Data”, ICML 2023 \n",
            "201 :  35 :  ERIC AND WENDY CHMIDT CENTER Diffusion model is an efficient density estimator Theorem ¥ (Distribution). Converge to the data distribution at the rate v (Fidelity). Deviation to the subspace is bounded by 9 (n-7'), ND WENDY DT CENTER OADS \n",
            "202 :  35 :  sen ERIC AND WENDY CHMIDT CENTER RIC AND WENDY J 7 SCHMIDT CENTER Diffusion model is an efficient density estimator Theorem ¥ (Distribution). Converge to the data distribution at the rate oO (n- 7a ) Y (Fidelity). Deviation to the subspace is bounded by 6(n-“»), Wang. \"Score Approximation, Estimation anc ICML 2023 Dimer \n",
            "203 :  35 :  ERIC AND WENDY CHMIDT CENTER CENTER Diffusion model is an efficient density estimator Theorem Vv (Distribution). Converge to the data distribution at the rate oO (n- 7a ) v (Fidelity). Deviation to the subspace is bounded by 9 (n-*#), timation an ICML 202: \n",
            "204 :  35 :  ERIC AND WENDY CHMIDT CENTER Diffusion model is an efficient density estimator Theorem ¥ (Distribution). Converge to the data distribution at the rate oO (n- 7a ) ROAD V (Fidelity). Deviation to the subspace is bounded by 9 (n-*\"»), Estimation an ta\", ICML 2023 \n",
            "205 :  35 :  gts ERIC AND WENDY CHMIDT CENTER Diffusion model is an efficient density estimator Theorem ¥ (Distribution). Converge to the data distribution at the rate oO (n- 7a ) v (Fidelity). Deviation to the subspace is bounded by 9 (n-*'»), Estimation an ICML 2023 \n",
            "206 :  35 :  ERIC AND WENDY SCHMIDT CENTER Diffusion model is an efficient density estimator Theorem ¥ (Distribution). Converge to the data distribution at the rate oO (n- x0 ) Y (Fidelity). Deviation to the subspace is bounded by 6 (n-=*), M.C, K. Huang, T. Zhao, M. Wang. \"Score Approximation, Estimation and Distribution Recovery of Diffusion Models on Low-Dimensional Data”, ICML 2023 \n",
            "207 :  35 :  ERIC AND WENDY SCHMIDT CENTER Diffusion model is an efficient density estimator Theorem ¥ (Distribution). Converge to the data distribution at the rate oO (n- 7a ) Y (Fidelity). Deviation to the subspace is bounded by 9 (n-7**=), M. C, K. Huang, T. Zhao, M. Wang. “Score Approximation, Estimation ani Distribution Recovery of Diffusion Moc n Low-Dimensional Data”, ICML 2023 \n",
            "208 :  35 :  ERIC AND WENDY CHMIDT CENTER Diffusion model is an efficient density estimator Theorem Vv (Distribution). Converge to the data distribution at the rate AD) } oO (n- 705 ) es Y (Fidelity). Deviation to the subspace is bounded by 6 (n=), >roximation, Estimation anc rensional Data”, ICML 202: \n",
            "209 :  35 :  ERIC AND WENDY CHMIDT CENTER SCHMIDT CENTER Diffusion model is an efficient density estimator Theorem Vv (Distribution). Converge to the data distribution at the rate oO (n- 7a ) Y (Fidelity). Deviation to the subspace is bounded by 6(n--»), Y Efficient in modeling data distributions v Adaptive to manifold structures M.C, K, Huang, T. Zhao, M. Wang. \"S Distribution Recovery of Diffusion Mc sroximation, Estimation anc rsional Data\", ICML 2023 \n",
            "210 :  35 :  ERIC AND WENDY CHMIDT CENTER Diffusion model is an efficient density estimator Theorem Vv (Distribution). Converge to the data distribution at the rate oO (n- mars) ) Y (Fidelity). Deviation to the subspace is bounded by 9 (\"=\"), Y Efficient in modeling data distributions J ¥ Adaptive to manifold structures Bie: M.C, K. Huang, T. Zhao, M. Wang. \"Score Approximation, Estimation and Distribution Recovery of Diffusion Models on Low-Dimensional Data”, ICML 2023 \n",
            "211 :  35 :  ERIC AND WENDY CHMIDT CENTER Diffusion model is an efficient density estimator Theorem ¥ (Distribution). Converge to the data distribution at the rate oO (n- 7a ) Y (Fidelity). Deviation to the subspace is bounded by 9 (n=), Y Efficient in modeling data distributions v Adaptive to manifold structures ng. “s Approximati timation an Dimensior ICML 2023 \n",
            "212 :  36 :  ERIC AND WENDY SCHMIDT CENTER 2, SCHMIDT CENTER Can we trust diffusion models? Forward (noising) process Now) ¥ Single ¥ step Protein ae > e AE structure xX AX xX, LAX Reverse (generative) process Gaussian ~ noise Q1: Can diffusion models learn the underlying data distribution? Q2: Can we control the generation of diffusion models towards specific objectives? How efficiently? -- credit: RFDiffusion from Baker's group, 2023 \n",
            "213 :  36 :  ERIC AND WENDY SCHMIDT CENTER AIDT CENTER Can we trust diffusion models? | a Forward (noising) process Now) ¥ Single ¥ step Protein eles —> oo structure ay e We r \\AX X, LAX Reverse (generative) process Gaussian ~ noise Q1: Can diffusion models learn the underlying data distribution? Q2: Can we control the generation of diffusion models towards specific objectives? How efficiently? -- credit: RFDiffusion from Baker's group, 2023 \n",
            "214 :  36 :  ERIC AND WENDY SCHMIDT CENTER Can we trust diffusion models? Forward (noising) process Now) ¥ Single ¥ step Gaussian ~ Protein noise ea Sees s+ RCS structure a e, We x, LA x, X. LAX Reverse (generative) process Q1: Can diffusion models learn the underlying data distribution? Q2: Can we control the generation of diffusion models towards specific objectives? How efficiently? -- credit: RFDiffusion from Baker's group, 2023 \n",
            "215 :  36 :  ERIC AND WENDY SCHMIDT CENTER 5) Senior ceNTeR | Can we trust diffusion models? Forward (noising) process Now) ¥ Single ¥ step Protein sai: < ee Ne We structure x LAX xX. LAX, Reverse (generative) process Gaussian ~ noise Q1: Can diffusion models learn the underlying data distribution? Q2: Can we control the generation of diffusion models towards specific objectives? How efficiently? -- credit: RFDiffusion from Baker's group, 2023 \n",
            "216 :  37 :  ERIC AND WENDY SCHMIDT CENTER Guiding diffusion model to generate new data of specific objective * RFDiffusion can generate binder design from a user-specified binding taenint Binding target Binder design ny i ey \n",
            "217 :  37 :  ERIC AND WENDY SCHMIDT CENTER Guiding diffusion model to generate new data of specific objective * RFDiffusion can generate binder design from a user-specified binding taeniat Binding target Binder design \n",
            "218 :  37 :  ERIC AND WENDY SCHMIDT CENTER Guiding diffusion model to generate new data of specific objective * RFDiffusion can generate binder design from a user-specified binding taenint Binding target Binder design ey ay ‘p Ww \n",
            "219 :  37 :  ERIC AND WENDY SCHMIDT CENTER Guiding diffusion model to generate new data of specific objective * RFDiffusion can generate binder design from a user-specified binding taennt Binding target Binder design y wy y a \n",
            "220 :  37 :  ERIC AND WENDY SCHMIDT CENTER ERIC AND WEN Guiding diffusion model to generate new data of specific objective * RFDiffusion can generate binder design from a user-specified binding Binding target Binder design eG Bs ne \n",
            "221 :  37 :  Guiding diffusion model to generate new data of specific objective * RFDiffusion can generate binder design from a user-specified binding tarnnt Binding target Binder design \n",
            "222 :  38 :  sm ERIC AND WENDY CHMIDT CENTER Adding Guidance to Diffusion Models * Class-conditioned sample generation Noise Conditional Score Brownian \n",
            "223 :  38 :  ERIC AND WENDY R RIC AND WENDY Zy SCHMIDT CENTER J eat Adding Guidance to Diffusion Models * Class-conditioned sample generation Noise Data f = og zm ee Xp +V lo dt+ dW, Conditional Score Brownian \n",
            "224 :  38 :  stis ERIC AND WENDY CHMIDT CENTER Adding Guidance to Diffusion Models * Class-conditioned sample generation Noise © < it Conditional Score Brownian \n",
            "225 :  39 :  Noise Brownian \n",
            "226 :  40 :  ERIC AND WENDY SCHMIDT CENTER Adding Guidance to Diffusion Models * Class-conditioned sample generation Noise Data Conditional Score Brownian \n",
            "227 :  40 :  ERIC AND WENDY R Adding Guidance to Diffusion Models * Class-conditioned sample generation Noise Data dt + dW, Conditional Score Brownian \n",
            "228 :  40 :  ERIC AND WENDY i R Adding Guidance to Diffusion Models * Class-conditioned sample generation Noise Data y = dog '— \"a Z axp = [5xr +o | ae+aW Conditional Score Brownian RIC AND WENDY SCHMIDT CENTER \n",
            "229 :  40 :  ERIC AND WENDY R Adding Guidance to Diffusion Models * Class-conditioned sample generation Noise Data | dt+dW, Conditional Score Brownian \n",
            "231 :  40 :  = ERIC AND WENDY CHMIDT CENTER Adding Guidance to Diffusion Models * Class-conditioned sample generation ix e| ij dt +aW, Conditional Score Brownian \n",
            "232 :  40 :  sen ERIC AND WENDY CHMIDT CENTER Adding Guidance to Diffusion Models * Class-conditioned sample generation EE BRO Noise Data — @—© —_@—® | = do = i dxf = Conditional Score Brownian Classifier-free guidance (Ho & Salimans, 2022) Classifier guidance (Dhariwal & Nichol, 2021) \n",
            "233 :  41 :  ERIC AND WENDY HMIDT CENTER ERIC AND WENDY SCHMIDT CENTER Towards Generative Optimization \n",
            "234 :  41 :  ERIC AND WENDY SCHMIDT CENTER RIC AND WENDY | SCHMIDT CENTER | Towards Generative Optimization \n",
            "235 :  41 :  ERIC AND WENDY SCHMIDT CENTER aMIDT CENTER Rethinking optimization as generative Al ee = ad max y = f(z) z~ P(X|Y = fmaz) Generative Al \n",
            "236 :  41 :  ERIC AND WENDY SCHMIDT CENTER Rethinking optimization as generative Al max y = f(x) x~ P(X|Y = fmaz) Generative Al \n",
            "237 :  41 :  RIC AND WENDY ERIC AND WENDY NDT CENTER Rethinking optimization as generative Al max y = f(x) a~ P(X|Y = fmaz) \n",
            "238 :  41 :  Rethinking optimization as generative Al max y = f(z) a P(X |4 =Menae) \n",
            "239 :  41 :  Rethinking optimization as generative Al pw P(XIY = ee Generative Al ——+() \n",
            "241 :  41 :  Rethinking optimization as generative Al max y = f(z) aw P(X |e =fran) \n",
            "242 :  41 :  RIC AND WENDY ERIC AND WENDY Rethinking optimization as generative Al ee max y = f(z) \n",
            "243 :  41 :  ERIC AND WENDY SCHMIDT CENTER Rethinking optimization as generative Al max y = f(z) x~ P(X|Y = fmaz) Generative Al \n",
            "244 :  41 :  ERIC AND WENDY SCHMIDT CENTER Rethinking optimization as generative Al maxy = f(z) r~ P(X|Y = fmaz) =a Generative Al \n",
            "245 :  41 :  ERIC AND WENDY SCHMIDT CENTER ERIC AND WENDY = SCHMIDT CENTER Rethinking optimization as generative Als max y = f(x) a~ P(X|Y = fmaz) High-D, nonconvex Structure hard to model \n",
            "246 :  41 :  ERIC AND WENDY SCHMIDT CENTER ERIC AND WENDY. SCHMIDT CENTER Rethinking optimization as generative Al max y = f(x) a~ P(X|Y = fmaz) High-D, nonconvex Structure hard to model \n",
            "247 :  41 :  ERIC AND WENDY SCHMIDT CENTER Rethinking optimization as generative Al max y = f(z) x~ P(X|Y = fmaz) Generative Al High-D, nonconvex Can we solve harder problems? Structure hard to model Why? \n",
            "248 :  41 :  ERIC AND WENDY SCHMIDT CENTER ERIC AND WENDY. ‘SCHMIDT CENTER Rethinking optimization as generative Al max y = f(x) z~ P(X|Y = fmaz) High-D, nonconvex Can we solve harder problems? Structure hard to model Why? \n",
            "249 :  42 :  ERIC AND WENDY SCHMIDT CENTER Problem Setup: Offline Optimization —.—_—_—_ «a * Given a training data set, generate new x \\ * Training data set Diabel = eon v= ~ F(x) +e} ip i=l > & is observation noise > pris (unknown) reward function \n",
            "250 :  42 :  ERIC AND WENDY SCHMIDT CENTER Problem Setup: Offline Optimization * Given a training data set, generate new x * Training data set Label Dunlabel = {xj} 527\" — Bel label Draber = {@i, ys = f* (ai) + es er\" = > & is observation noise > pris (unknown) reward function \n",
            "251 :  42 :  ERIC AND WENDY R Problem Setup: Offline Optimization * Given a training data set, generate new x * Training data set Label Slay ig a Z label Diaber = {#i, yi = f* (wi) + ce }e\" > & is observation noise > fis (unknown) reward function \n",
            "252 :  42 :  ERIC AND WENDY R Problem Setup: Offline Optimization * Given a training data set, generate new x * Training data set Dunlabe = {25 }524\"\" Diabei = {xi, yi = f* (wi) + a }e\" > & is observation noise > pris (unknown) reward function \n",
            "253 :  42 :  Problem Setup: Offline Optimization * Given a training data set, generate new x * Training data set Label Diabel = {xi, u= f (24) aT 7 a > & is observation noise > pris (unknown) reward function \n",
            "254 :  42 :  ERIC AND WENDY SCHMIDT CENTER Problem Setup: Offline Optimization * Given a training data set, generate new x * Training data set Label Dunlabet = {25 }j21°\"\" Bas f* (xi) + eg} eens label Diba = (07,4: = > & is observation noise > ftis (unknown) reward function Examples: \n",
            "255 :  42 :  ERIC AND WENDY SCHMIDT CENTER SCHMIDT CENTER Problem Setup: Offline Optimization * Given a training data set, generate new x * Training data set Label pat label Dunlabel = {xj} 527°\" Diabel Diabei = {xi, yi = f* (ai) + See\" > & is observation noise > pris (unknown) reward function Examples: \n",
            "256 :  42 :  ERIC AND WENDY SCHMIDT CENTER Problem Setup: Offline Optimization * Given a training data set, generate new x * Training data set F= BROWD) x Label oo ae a label = f* (wi) + a} = Diabel = (0:, 91 = > & is observation noise > fis (unknown) reward function Examples: \n",
            "257 :  42 :  ERIC AND WENDY SCHMIDT CENTER Problem Setup: Offline Optimization * Given a training data set, generate new x * Training data set EE BROAD Label = f(a) + ea} = Diabel = (es Y= > & is observation noise > pris (unknown) reward function Examples: \n",
            "258 :  43 :  ERIC AND WENDY SCHMIDT CENTER SCHMIDT CENTER Problem Setup: Offline Optimization =. = * Given a training data set, generate new x * Training data set _— Label BROAD Dinadel = {x; peace. Diaber = {xi, yi = f* (ai) + ce }e\" > & is observation noise > pris (unknown) reward function Examples: Qa large collection of unlabeled protein structures; only a few has measured properties. \n",
            "259 :  44 :  ERIC AND WENDY SCHMIDT CENTER Problem Setup: Offline Optimization * Given a training data set, generate new x * Training data set Label ROAD Duniabel = {25 }524\"\" Diaber = {@i, yi = f* (ai) + ce }Ee\" > & is observation noise > pris (unknown) reward function Examples: Qa large collection of unlabeled protein structures; only a few has measured properties. \n",
            "260 :  45 :  ERIC AND WENDY = SCHMIDT CENTER Problem Setup: Offline Optimization * Given a training data set, generate new x * Training data set Label Bos ie at : z label Diaber = {xi, yi = f* (wi) + ce }eEe\" > & is observation noise > pris (unknown) reward function Examples: Qa large collection of unlabeled protein structures; only a few has measured properties. \n",
            "261 :  46 :  ERIC AND WENDY SCHMIDT CENTER Problem Setup: Offline Optimization * Given a training data set, generate new x * Training data set Label Tunlabel Duntabel = {5} 527 Diabei = {@i, yi = f* (ai) + ce }Ee\" > & is observation noise > fis (unknown) reward function Examples: Qa large collection of unlabeled protein structures; only a few has measured properties. Q RNA-seq atlas, with only few annotations \n",
            "262 :  46 :  Problem Setup: Offline Optimization * Given a training data set, generate new x * Training data set Label Diabel = {2i, yi = f* (ai) + a ee > € is observation noise > pris (unknown) reward function Examples: Q alarge collection of unlabeled protein structures; only a few has measured properties. Q RNA-seq atlas, with only few annotations \n",
            "263 :  46 :  Problem Setup: Offline Optimization * Given a training data set, generate new xr * Training data set Label Diabet = {2i, yi = f* (as) + cc Hp > € is observation noise > pris (unknown) reward function Examples: Q a large collection of unlabeled protein structures; only a few has measured properties. Q RNA-seq atlas, with only few annotations \n",
            "264 :  46 :  Problem Setup: Offline Optimization * Given a training data set, generate new x * Training data set Label Diabel = {2i, yi = f* (ai) + a ee 65 P P > & iS observation noise f P Off-policy bandit problem > fils (unknown) reward function (Jin et al., 2021; Nguyen-Tang et al., 2021) Examples: Q a large collection of unlabeled protein structures; only a few has measured properties. Q RNA-seq atlas, with only few annotations \n",
            "265 :  47 :  ERIC AND WENDY CHMIDT CENTER Problem Setup: Offline Optimization * Given a training data set, generate new x * Training data set Label Munlabe Duntabel = {&j he peel . one Cintel Diadel Diabet = {xi, yi = f* (wi) + FY > & is observation noise of i silt wank! > pris (unknown) reward function (in et al,, 2021; NguyenTang et al., 2021) Examples: Qa large collection of unlabeled protein structures; only a few has measured properties. QO RNA-seq atlas, with only few annotations —BROAD i. \n",
            "266 :  47 :  ERIC AND WENDY CHMIDT CENTER Problem Setup: Offline Optimization * Given a training data set, generate new * Training data set Dunlabel = {25 }525\"\" Diabei = {@i, yi = f* (ai) + See\" > & is observation noise > pris (unknown) reward function Examples: Qa large collection of unlabeled protein structures; only a few has measured properties. Q RNA-seq atlas, with only few annotations (Jin et al., 2021; Nguye nTang et al, 2021) SCHMIDT CENTER BROAD yd \n",
            "267 :  48 :  cn ERIC AND WENDY CHMIDT CENTER We propose a meta-algorithm: Reward- conditioned diffusion model Label Reward Model =. Step 1: Reward Learning SCHMIDT CENTER Reward-directed conditional diffusio Provable distribution estimation and reward improvement. NeurlPS, 2022 \n",
            "268 :  48 :  ERIC AND WENDY CHMIDT CENTER We propose a meta-algorithm: Reward- conditioned diffusion model Label Reward Model =. Step 1: Reward Learning Reward-directed conditional diffusion: Provable distribution estimation and reward improvement. NeurIPS, 2022 \n",
            "270 :  48 :  ERIC AND WENDY CHMIDT CENTER We propose a meta-algorithm: Reward- conditioned diffusion model Label Reward Model =, Step 1: Reward Learning Reward-directed conditional diffusion: Provable distribution estimation and reward improvement. NeurIPS, 2022 \n",
            "271 :  48 :  ERIC AND WENDY CHMIDT CENTER We propose a meta-algorithm: Reward- conditioned diffusion model Label Reward Model = Step 1: Reward Learning Reward-directed conditional diffusion: Provable distribution estimation and reward improvement. NeurIPS, 2022 \n",
            "272 :  48 :  We propose a meta-algorithm: Reward- conditioned diffusion model Label Pseudo-label = Reward Model an Step 1: Reward Learning Step 2: Pseudo Labeling Reward-directed conditional diffusion: Provable distribution estimation and reward improvement. NeurIPS, 2022 34 \n",
            "273 :  49 :  ERIC AND WENDY R We propose a meta-algorithm: Reward- conditioned diffusion model pane eat Label Pseudo-label nm Reward Model Step 1: Reward Learning Step 2: Pseudo Labeling onditional diffusion: Provable distribution estimation and reward improvement. NeurlPS, 2022 \n",
            "274 :  49 :  ERIC AND WENDY R We propose a meta-algorithm: Reward- conditioned diffusion model Label R ee Pseudo-label — eward Model =. aaa — Step 1: Reward Learning Step 2: Pseudo Labeling Conditional Diffusion Step 3: Conditional Diffusion Training Reward-directed conditional diffusio ple distribution estimation and reward improvement. NeurlPS, 2022 \n",
            "275 :  49 :  We propose a meta-algorithm: Reward- conditioned diffusion model Label Pseudo-label Reward Model oo = Step 1: Reward Learning Step 2: Pseudo Labeling Conditional Score O P(- | reward) Conditional Diffusion Step 3: Conditional Diffusion Training Reward-directed conditional diffusion: Provable distribution estimation and reward improvement. NeurlPS, 2022 34 \n",
            "276 :  50 :  Pseudo-label esl 2: Pseudo Labeling Step 3 Conditional Diffusion Ty, \n",
            "277 :  51 :  We propose a meta-algorithm: Reward- conditioned diffusion model Label Pseudo-label > Reward Model a Step 1: Reward Learning Step 2: Pseudo Labeling Conditional Score reward = a <a) Conditional Diffusion Step 3: Conditional Diffusion Training Step 4: Guided Generation Reward-directed conditional diffusion: Provable distribution estimation and reward improvement. NeurIPS, 2022 34 \n",
            "278 :  52 :  ERIC AND WENDY R We propose a meta-algorithm: Reward- conditioned diffusion model ae Label - en Pseudo-label — eward Model =. ana — BROAD) Step 1: Reward Learning Step 2: Pseudo Labeling rewarc ard) ——— Diffusion Model | as : Guided Generation le distribution estimation and reward improvement. NeurlPS, 2022 Step 3: Conditional Diffusion Tr onditional diffusion: P ard-direc \n",
            "279 :  53 :  ERIC AND WENDY R We propose a meta-algorithm: Reward- conditioned diffusion model Label — Reward Model = Step 1: Reward Learning ‘onditional diffusi Pseudo-label — Step 2: Pseudo Labeling Diffusion Model —| ion: Prova le distribution estima : Guided Generation tion and reward improvement. NeurIPS, 2022 scrim CENT RI ‘BROAD \n",
            "280 :  54 :  ERIC AND WENDY R We propose a meta-algorithm: Reward- conditioned diffusion model Label Reward Model =. Step 1: Reward Learning Conditional Diffusion Step 3: Conditional Diffusion Tr nditional diffu Pseudo-label — Step 2: Pseudo Labeling ————— Diffusion Model —_ i ae : Guided Generation ment. NeurlPS, 2022 le distribution estimation and reward imp! \n",
            "281 :  55 :  ERIC AND WENDY CHMIDT CENTER Reward-condition diffusion for generative optimization \n",
            "282 :  55 :  ERIC AND WENDY R Reward-condition diffusion for generative optimization ‘SCHMIDT CENTER Question: How far are we from the target objective value? How good are new samples? \n",
            "283 :  55 :  ERIC AND WENDY R Reward-condition diffusion for generative optimization Question: How far are we from the target objective value? How good are new samples? ‘SCHMIDT CENTER. \n",
            "284 :  55 :  ERIC AND WENDY R Reward-condition diffusion for generative optimization Question: How far are we from the target objective value? How good are new samples? Target Training Diabet SCHMIDT CENTER \n",
            "285 :  55 :  ERIC AND WENDY R Reward-condition diffusion for generative optimization SCHMIDT CENTER | Question: How far are we from the target objective value? How good are new samples? Target r 19 Diabet \n",
            "286 :  55 :  ERIC AND WENDY R Reward-condition diffusion for generative optimization sen ic \\J BROAD: Stop 1: Reward Learning Step 2: Peoudo Labeling Question: How far are we from the target objective value? How good are new samples? Target r Diabet \n",
            "287 :  55 :  ERIC AND WENDY R Reward-condition diffusion for generative optimization Question: How far are we from the target objective value? How good are new samples? Target Tr 19 Diabet | \n",
            "288 :  55 :  ERIC AND WENDY R Reward-condition diffusion for generative optimization Question: How far are we from the target objective value? How good are new samples? Target Training Diabel \n",
            "289 :  55 :  ERIC AND WENDY R Reward-condition diffusion for generative optimization Question: How far are we from the target objective value? How good are new samples? Target Training Diabet raining \n",
            "290 :  56 :  ERIC AND WENDY SCHMIDT CENTER Optimization theory (linear model) Theorem v New samples are high-fidenty. The sub-optimality satisfies aorta = Trace (8;2.) -4{ Peelerased Pe cee ate Munlabel where 8, =(X7X +AJ)/napafor Xx the data matrix, \\>0, and ,is the covariance matrix of P,(- | reward = a). H. Yuan, K. Huang, C. Ni, M. C, M. Wang. “Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement”, NeurlPS 2023 \n",
            "291 :  56 :  ERIC AND WENDY SCHMIDT CENTER Optimization theory (linear model) Theorem v¥ New samples are high-fidenty. The sub-optimality satisfies Sub0pt(a) = O ( Trace (8572.) Jaf ee +min{a, d} - ee ee ) J ‘abel Nunlabel where 8, =(X7X +AJ)/napafor x the data matrix, \\>0, and 5, is the covariance matrix of P,(- | reward = a). H. Yuan, K. Huang, C. Ni, M. C, M. Wang. “Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement”, NeurIPS 2023 \n",
            "292 :  57 :  ERIC AND WENDY R Optimization theory (linear model) rt cee Theorem \\ v New samples are high-fidenty. The sub-optimality satisfies aa 7 3RO/ | a: nape.) _ a di log (Mabel) penin(aee oy Munlabel Sub0pt(a) = O ( Trace (83?Ba) Mabel where 8, = (X7X +AJ)/napafor Xx the data matrix, \\>0, and =, is the covariance matrix of P,(- | reward = a). \n",
            "293 :  57 :  ERIC AND WENDY SCHMIDT CENTER Optimization theory (linear model) Theorem v New samples are high-fidenty. The sub-optimality satisfies Sub0pt(a) = o ( Trace (8512.) Jaf seal) -+ min{a, d} - oe vee} a) J a ee ere eee Nunlabel where 8, =(X7X +AJ)/napafor x the data matrix, \\>0, and 5, is the covariance matrix of P,(- | reward = a). H. Yuan, K. Huang, C. Ni, M. C, M. Wang. “Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement”, NeurlPS 2023 \n",
            "294 :  57 :  ERIC AND WENDY CHMIDT CENTER Optimization theory (linear model) Theorem v New samples are high-fidenty. The sub-optimality satisfies dlog(n 4 ? Sub0pt(a Trace ( $315.) dlog (Mader) label) +H . Mabel : ' . where &, = (X7X +AJ)/napafor Xx the data matrix, \\>0, and 5, is the covariance matrix of P,(- | reward = a). \n",
            "295 :  58 :  : ERIC AND WENDY MIDT CENTER Optimization theory (linear model) Theorem v New samples are high-fidenty. The sub- ipa satisfies where &, = (XX +AJ)/nuapa for X the data een d>0, and S.is the covariance matrix of P,(- | reward = a). “ Match optimal off-policy bandit learning with known representations (Jin et al., 2021; Nguyen-Tang et al., 2021) H. Yuan, K. Huang, C. Ni, M. C, M. Wang. “Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement”, NeurIPS 2023 \n",
            "296 :  58 :  : ERIC AND WENDY MIDT CENTER Optimization theory (linear model) Theorem v New a aes are high-fidenty. The sub- penn satisfies where S, = (X7X +AJ)/mapa for X the data Sanity d>0, and Sis the covariance matrix of P,(- | reward = a). * Match optimal off-policy bandit learning with known representations (Jin et al., 2021; Nguyen-Tang et al., 2021) H. Yuan, K. Huang, C. Ni, M. C, M. Wang. “Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement”, NeurlPS 2023 \n",
            "297 :  58 :  Optimization theory (linear model) v New samples are high-fidenty. The sub-optimality satisfies Sub0pt(a) = O | Trace (83?Ba) where &, = (X7X +AJ)/napafor x the data matrix, \\>0, and 5, is the covariance matrix of P,(- | reward = a). representations (Jin et al., 2021; Nguyen-Tang et al., 2021) SExwibr CENTER f \n",
            "298 :  58 :  ERIC AND WENDY R Optimization theory (linear model) Theorem v¥ New samples are high-fidenty. The sub-optimality satisfies Sub0pt(a) = O Trace (8722) where 8, = (X7X +AJ)/mapafor Xx the data matrix, \\>0, and =, is the covariance matrix of P,(- | reward = a). H. Yuan, K. Huang, ¢ representations (Jin et al., 2021; Nguyen-Tang et al., 2021) AND WENDY CHMIDT CENTER RG BROAD \n",
            "299 :  58 :  ERIC AND WENDY R Optimization theory (linear model) agp Theorem NN v¥ New samples are high-fidenty. The sub-optimality satisfies f =i ROAD dlog(mavei) Ms ) ; Nabel : ' x ; Sub0pt(a) = O | Trace (83724) where 8, =(X7X +AJ)/napafor x the data matrix, \\>0, and =, is the covariance matrix of P,(- | reward = a). I icy k i ing with known representations (Jin et al., 2021; Nguyen-Tang et al., 2021) \n",
            "300 :  59 :  ERIC AND WENDY R Advantages of Generative Optimization VY Meta algorithm provably generates samples of high reward and fidelity, in nonparametric settings. = -7— —aéan subopt(a) = 0 (aa) mi + (a) mse J \n",
            "301 :  59 :  ERIC AND WENDY R Advantages of Generative Optimization VY Meta algorithm provably generates samples of high reward and fidelity, in nonparametric settings. a oe —aéan subopt(a) = 6 (x(a) ml + (a) mae J to learn structures from unlabeled data new samples maintain data’s intrinsic structures v provable optimality H. Yuan, K. Huang, C. Ni, M. C rovable Distribution E d Improvement”, NeurIPS 2023 \n",
            "302 :  60 :  Advantages of Generative Optimization Y Meta algorithm provably generates samples of high reward and fidelity, in nonparametric settings. ~ — a —_— 2 subopt(a) = 0 (1(a) madd + m2(0) mas ) v Leverages pretraining to learn structures from unlabeled data ¥ High-fidelity: new samples maintain data’s intrinsic structures ¥ Efficient optimizer: provable optimality —Ss -- H. Yuan, K. Huang, C. Ni, M. C, M. Wang. “Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement\", NeurlPS 2023 37 \n",
            "303 :  60 :  Advantages of Generative Optimization Y Meta algorithm provably generates samples of high reward and fidelity, in nonparametric settings. ~ a — 2 subopt(a) = 0 (1(a)- madd + m2(0) mae ) v Leverages pretraining to learn structures from unlabeled data ¥ High-fidelity: new samples maintain data’s intrinsic structures ¥ Efficient optimizer: provable optimality —Ss -- H. Yuan, K. Huang, C. Ni, M. C, M. Wang. “Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement\", NeurlPS 2023 37 \n",
            "304 :  60 :  ERIC AND WENDY R Advantages of Generative Optimization VY Meta algorithm provably generates samples of high reward and fidelity, in nonparametric settings. = -7— —adésn subopt(a) = 0 (1a) mi + (a) ase) to learn structures from unlabeled data new samples maintain data’s intrinsic structures v provable optimality \n",
            "305 :  60 :  ERIC AND WENDY R Advantages of Generative Optimization VY Meta algorithm provably generates samples of high reward and fidelity, in nonparametric settings. 2 ae —agéan subopt(a) = 6 (1a) ml + (a) mae ) to learn structures from unlabeled data new samples maintain data’s intrinsic structures v provable optimality Huang, C.N vable Distrib \n",
            "306 :  60 :  ERIC AND WENDY R Advantages of Generative Optimization VY Meta algorithm provably generates samples of high reward and fidelity, | in nonparametric settings. 7 x + =a subopt(a) = 0 (saa) mi + (a) Mase J J to learn structures from unlabeled data new samples maintain data’s intrinsic structures v provable optimality H. Yuan, K. Huang, C. Ni, M. C Provable Distribution \n",
            "307 :  60 :  ERIC AND WENDY R Advantages of Generative Optimization VY Meta algorithm provably generates samples of high reward and fidelity, in nonparametric settings. = a —aéan subope(a) =O (ria) ml + aa) Mase J to learn structures from unlabeled data new samples maintain data’s intrinsic structures v provable optimality \n",
            "308 :  61 :  ERIC AND WENDY CHMIDT CENTER Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation 2 Severe Extrapolation: am Large Distebution Shift ¢ Quality Degradati Good Fidelity B15, Good Data Coverage: Monotone Quality Improvement Distribution Shift Target Reward Value « “Target Reward Value a “Target Reward Value a \n",
            "309 :  61 :  ERIC AND WENDY CHMIDT CENTER Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation Severe Extrapolation: Longe Distribution Shift \" ! Quality Degradation Good Fideley Eis Good Data Coverage: Bow Monotone Quality Improvement Distribution Shift “Target Reward Value a” 8 Farget Reward Value a Target Reward Value « \n",
            "310 :  61 :  stig ERIC AND WENDY CHMIDT CENTER Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation Severe Extrapolation: Longe Distribution Shift Quality Degradation ° ‘sag, 600d Fidelity 21s, Good Data Coverage: Distribution Shift Monotone Quality Improvement “Target Reward Value a” 8 Farget Reward Value a Target Reward Value « \n",
            "311 :  61 :  ERIC AND WENDY R Increasing target reward always leads to better samples? SCHMIDT CENTE i * Over extrapolation can lead to compounding off-support error and low data quality = BROAD * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation 2 Severe Extrapolation: | cp Lange Distribution Shift cous g aly Degradation” Good Fidelity dis a Good Data Coverage: Monotone Quality Improvement Distribution Shift Target Reward Value « _ f° \"Target Reward Value a) Target Reward Value a \n",
            "312 :  61 :  ERIC AND WENDY R Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation 2s: Severe Extrapolation: Longe Distribution Shift I | g” | Quality Degradation Good Fidelity $20 T poo Bas i Bis, Goud Dat coverage) 5 Distribution Shift Monotone Quality 2 Improvement Target Reward Value « “Target Reward Value a” °° Target Reward Value a \n",
            "313 :  61 :  ERIC AND WENDY i R Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation 2s) Severe Extrapolation: Longe Distribution Shift ! i =\" | (aaa DeeraaNHE”” ood Fatty B20 ; B40 dis j LL aaa Distribution Shift Monotone Quality nm | Improvement ig Xo x0 d; 30 oo To, 20 x0 40 re, 20 x0 «0 ‘Target Reward Value a 7 ‘Target Reward Value « ‘Target Reward Value « \n",
            "314 :  61 :  ERIC AND WENDY CHMIDT CENTER Increasing target reward always leads to better samples? SCHMIDT * Over extrapolation can lead to compounding off-support error and low data quality + Numerical example: FEBRO SS Distribution Shift Generated Average Reward Off-support Deviation 2s. Severe Extrapolation: | Longe Distribution Shift cas Quality Degradation Good Fidelity $20 ’ i Ba oss 21s, Good Data Coverage: Foo Distribution Shift Monotone Quality a Improvement i : 1 3 y ‘Target Reward Value « “Target Reward Value « “Target Reward Value « \n",
            "315 :  61 :  ERIC AND WENDY i R Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: ion Shift Generated Average Reward Off-support Deviation ! 25 Severe Extrapolation: | Large Distribution Sift cas =? Nv iia Degradation” Geog Fidelity $7 ! g i. gos 21s Good Data Coverage: | Distribution Shift Monotone Quality Leal ( Improvement ig Xo x9 i; 50 oo 10, 20 x0 ao 19, 20 xo «0 ‘Target Reward Value « i ‘Target Reward Value « ‘Target Reward Value « \n",
            "316 :  62 :  ERIC AND WENDY SCHMIDT CENTER Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation 2s Severe Extrapolation: | Lorge Distribution Shit ous i Quilty Degradation” Gogg Fidelity as 8 dus i os Eis Good Data Coverage: Foo Distribution Shift Monotone Quality 5 0.25: 12 | Improvement ig zo x0 i; 30 oo 10, 20 x0 ao 0 ry 19, 20 x0 «0 ‘Target Reward Value « * ‘Target Reward Value ° ‘Target Reward Value « \n",
            "317 :  62 :  ERIC AND WENDY R Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation Quality Degradation Good Fidelity pe Distribution Shite us X 1s Good Data Coverage: Foa0 L Distribution Shift Monotone Quality Sea Improvement “Target Reward Value a. “Target Reward Value a “Target Reward Value @ \n",
            "318 :  62 :  ERIC AND WENDY R Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation g Qiiality Degradation” —Goog Fidelity $20 ee 3s soos 21s Good Data Coverage: Fo Distribution Shift Monotone Quality a | Improvement a <3 we 3g a5 oe 3g ea 80 ‘Target’ Reward Value a” ararpst Rowen Vane Gn Trae? Reward Vane \n",
            "319 :  63 :  ERIC AND WENDY SCHMIDT CENTER Reward improves as target value increases ‘Quaiity degrades when overly extrapolated Watch for distribution shift! \n",
            "320 :  63 :  ERIC AND WENDY SCHMIDT CENTER Numerical experiment: image synthesis ca | Reward improves as target value increases ‘Quality degrades when overly extrapolated Watch for distribution shift! \n",
            "321 :  63 :  ERIC AND WENDY <2. SCHMIDT CENTER Reward improves as target value increases ‘Quality degrades when overly extrapolated Watch for distribution shift! \n",
            "322 :  63 :  ERIC AND WENDY <2), SCHMIDT CENTER ERIC AND WENDY ‘SCHMIDT CENTER Reward improves as target value increases ‘Quaiity degrades when overly extrapolated Watch for distribution shift! \n",
            "324 :  63 :  ERIC AND WENDY 235: SCHMIDT CENTER Reward improves as target value increases Watch for distribution shift! \n",
            "325 :  64 :  gts: ERIC AND WENDY CHMIDT CENTER Numerical experiment with optimal control: Generate state trajectories conditioned on high value rs Target reward Hopper Control Avg. reward Diffusion optimizer is comparable to best RL algorithm in some control tasks \n",
            "326 :  64 :  ERIC AND WENDY CHMIDT CENTER Numerical experiment with optimal control: Generate state trajectories conditioned on high value Target reward Hopper Control Avg. reward Diffusion optimizer is comparable to best RL algorithm in some control tasks \n",
            "327 :  64 :  ERIC AND WENDY CHMIDT CENTER Numerical experiment with optimal control: Generate state trajectories conditioned on high value DT CENTER . | ai 3OAD Target reward Diffusion optimizer is comparable to best RL algorithm in some control tasks Hopper Control Avg. reward \n",
            "328 :  64 :  ERIC AND WENDY SCHMIDT CENTER Numerical experiment with optimal control: Generate state trajectories conditioned on high value Target reward Hopper Control Avg. reward Diffusion optimizer is comparable to best RL algorithm in some control tasks \n",
            "329 :  64 :  ERIC AND WENDY CHMIDT CENTER Numerical experiment with optimal control: Generate state trajectories conditioned on high value Target reward Hopper Control Avg. reward Diffusion optimizer is comparable to best RL algorithm in some control tasks \n",
            "331 :  64 :  ERIC AND WENDY CHMIDT CENTER WENDY CENTER Numerical experiment with optimal control: Generate state trajectories conditioned on high value Target reward Hopper Control Avg. reward Diffusion optimizer is comparable to best RL algorithm in some control tasks \n",
            "332 :  64 :  sian ERIC AND WENDY CHMIDT CENTER Numerical experiment with optimal control: Generate state trajectories conditioned on high value Target reward Hopper Control Avg. reward Diffusion optimizer is comparable to best RL algorithm in some control tasks \n",
            "333 :  64 :  scm ERIC AND WENDY CHMIDT CENTER Numerical experiment with optimal control: Generate state trajectories conditioned on high value al i Target reward Hopper Control Avg. reward Diffusion optimizer is comparable to best RL algorithm in some control tasks \n",
            "334 :  64 :  Numerical experiment with optimal control: Generate state trajectories conditioned on high value Hopper Control eof Match SOTA MOReL anus Avg. reward FSF pe pips? kas 4 , Tart reward Surprise: Diffusion optimizer is comparable to best RL algorithm in some control tasks \n",
            "335 :  65 :  ERIC AND WENDY SCHMIDT CENTER Numerical experiment with optimal control: Generate state trajectories conditioned on high value } yT Match SOTA / A I |. Diffusion optimizer is comparable to best RL algorithm in some control tasks Hopper Control Avg. reward ey Target reward | \n",
            "336 :  65 :  ERIC AND WENDY CHMIDT CENTER Numerical experiment with optimal control: Generate state trajectories conditioned on high value Target reward scrmor cent a! Hopper Control Avg. reward Diffusion optimizer is comparable to best RL algorithm in some control tasks \n",
            "337 :  65 :  ERIC AND WENDY CHMIDT CENTER Numerical experiment with optimal control: Generate state trajectories conditioned on high value Target reward Hopper Control Avg. reward Diffusion optimizer is comparable to best RL algorithm in some control tasks \n",
            "338 :  65 :  ERIC AND WENDY CHMIDT CENTER Numerical experiment with optimal control: Generate se ene state trajectories conditioned on high value Target reward Hopper Control Avg. reward Diffusion optimizer is comparable to best RL algorithm in some control tasks \n",
            "339 :  65 :  stig ERIC AND WENDY CHMIDT CENTER Numerical experiment with optimal control: Generate state trajectories conditioned on high value Target reward Hopper Control Avg. reward Diffusion optimizer is comparable to best RL algorithm in some control tasks \n",
            "340 :  65 :  ERIC AND WENDY CHMIDT CENTER Numerical experiment with optimal control: Generate state trajectories conditioned on high value Hopper Control Avg. reward Target reward Diffusion optimizer is comparable to best RL algorithm in some control tasks \n",
            "341 :  65 :  Numerical experiment with optimal control: Generate state trajectories conditioned on high value Hopper Control = Match SOTA MOReL ius ee rewa rd at il l |. Targat reward Surprise: Diffusion optimizer is comparable to best RL algorithm in some control tasks \n",
            "342 :  65 :  Numerical experiment with optimal control: Generate state trajectories conditioned on high value Hopper Control tof Match SOTA MORoL user » ep ° ” - = Avg. reward pp Target reward Surprise: Diffusion optimizer is comparable to best RL algorithm in some control tasks \n",
            "343 :  66 :  sts ERIC AND WENDY CHMIDT CENTER Numerical experiment with optimal control: Generate state trajectories conditioned on high value Target reward Hopper Control Avg. reward Diffusion optimizer is comparable to best RL algorithm in some control tasks \n",
            "344 :  66 :  ERIC AND WENDY CHMIDT CENTER Numerical experiment with optimal control: Generate ea ano wena state trajectories conditioned on high value Target reward Hopper Control Avg. reward Diffusion optimizer is comparable to best RL algorithm in some control tasks \n",
            "345 :  66 :  ERIC AND WENDY CHMIDT CENTER | SCHMIDT CENTER i, Numerical experiment with optimal control: Generate state trajectories conditioned on high value Target reward Hopper Control Avg. reward Diffusion optimizer is comparable to best RL algorithm in some control tasks \n",
            "346 :  66 :  ERIC AND WENDY SCHMIDT CENTER Numerical experiment with optimal control: Generate state trajectories conditioned on high value Target reward Hopper Control EEBROAD Avg. reward Diffusion optimizer is comparable to best RL algorithm in some control tasks \n",
            "347 :  67 :  ERIC AND WENDY CHMIDT CENTER CENTER Anew modality: Generative Optimization Guidance SOee \n",
            "348 :  67 :  stim ERIC AND WENDY CHMIDT CENTER Anew modality: Generative Optimization IC AND WENDY | SCHMIDT CENTER Guidance Oe \n",
            "349 :  67 :  ERIC AND WENDY CHMIDT CENTER A new modality: Generative Optimization Conditional Diffusion Finetune ; Evaluation Guidance Pretraining + Online \n",
            "350 :  67 :  scm ERIC AND WENDY CHMIDT CENTER jj ScHMIOT C centeR E Anew modality: Generative Optimization Conditional Diffusion Finetune - Evaluation Guidance Oe Pretraining + Online \n",
            "351 :  67 :  ERIC AND WENDY R Anew modality: Generative Optimization Conditional Diffusion Finetune Evaluation Guidance Sous ¥ High-Dim structured data Pretraining + Online \n",
            "352 :  67 :  ERIC AND WENDY R A new modality: Generative Optimization Conditional Diffusion Finetune Evaluation Guidance SOee ¥ High-Dim structured data Y Life-long adaptation => | Pretraining y Online \n",
            "353 :  67 :  ERIC AND WENDY CHMIDT CENTER A new modality: Generative Optimization Guidance Conditional Diffusion Pretraining + Online ¥ High-Dim structured data Y Life-long adaptation v Sample efficiency ote \n",
            "354 :  68 :  ERIC AND WENDY SCHMIDT CENTER An analogy to guided diffusion: -d directed evolution for Directed _ Evolution amplificatio® \n",
            "355 :  68 :  ERIC AND WENDY SCHMIDT CENTER ANG ; ey » $ Directed : Evolution : & amplificatio® \n",
            "356 :  68 :  ERIC AND WENDY SCHMIDT CENTER An analogy to guided diffusion: g OS g Directed % Evolution ; parent ©} amplificatio® \n",
            "357 :  68 :  ERIC AND WENDY SCHMIDT CENTER g Se Ree ; o. e V 5 Directed i _ Evolution ay ingore ‘ 8 ampiificatio® \n",
            "358 :  68 :  ERIC AND WENDY SCHMIDT CENTER An analogy to guided diffusion: suided directed evolution for | Directed & _ Evolution i amplificatio™ \n",
            "359 :  68 :  ERIC AND WENDY SCHMIDT CENTER An analogy to guided diffusion: & & ; 2 Directed Evolution amplificatio® \n",
            "360 :  68 :  ERIC AND WENDY SCHMIDT CENTER An analogy to guided diffusion: sided directed evolution for J me g Directed % Evolution i & parent > @mplificatio® \n",
            "362 :  69 :  ERIC AND WENDY SCHMIDT CENTER Thompson Sampling-guided directed evolution provably explores sequence space mutation recombination Directed Mutation Crossover Selection Hi Yuan, C Ni, H Wang, X Zhang, L Cong, C Szepesvari, M Wang, Bandit Theory and Thompson Sampling-Guided Directed Evolution for Se e Optimization. NeurIPS 2022. \n",
            "363 :  69 :  ERIC AND WENDY SCHMIDT CENTER Thompson Sampling-guided directed evolution provably explores sequence space mutation recombination Directed Mutation Crossover Selection Hi Yuan, C Ni, H Wang, X Zhang, L Cong, C Szepesvari, M Wang, Bandit Theory and Thompson Sampling-Guided Directed Evolution for Sequer imization. NeurIPS 2022. ~ \n",
            "364 :  69 :  Thompson Sampling-guided directed evolution provably explores sequence space mutation recombination Targeted sites ¥ y Directed Mutation Crossover Selection ‘Original seq. fi 1 ro ) | =>! S; Mutated seq. Possible’ehitdren -- Hi Yuan, C Ni, H Wang, X Zhang, L Cong, C Szepesvari, M Wang. Bandit Theory and Thompson Sampling-Guided Directed Evolution for Sequence Optimization. NeurIPS 2022. \n",
            "365 :  70 :  ERIC AND WENDY SCHMIDT CENTER Thompson Sampling-guided directed evolution provably explores sequence space mutation recombination || directed mutation crossover selection Hi Yuan, C Ni, H Wang, X Zhang, L Bandit Theory and Thompson Sampling-Guided Directed Evolution for Sequence ( ng, C Szepesvari, M Wang imization. NeurIPS 2022 \n",
            "366 :  71 :  Thompson Sampling-guided directed evolution provably explores sequence space ERIC AND WENDY SCHMIDT CENTER Si-1 = Crossover Selection rm & —. Thompson_Sampling ¢ ——— Posterior Model |Sachupdare| Dataset mutation recombination } Hi Yuan, C Ni, H Wang, X Zhang, L Cong, C Szepesvari, M Wang Bandit Theory and Thompson Sampling-Guided Directed Evolution for Sequence Optimization. NeurIPS 2022 \n",
            "367 :  72 :  ERIC AND WENDY SCHMIDT CENTER Thompson Sampling-guided directed evolution provably explores sequence space mutation recombination || oirected mutation crossover. selection | Thompson_Sampling 2 ected Munaton ——— Posterior 1 crenore Stein +——]_ Moder | Sacnopaare] Dataset Hi Yuan, C Ni, H Wang, X Zha Bandit Theory and Thompson Sampling-Guided Directed Evolution for Sec g, L Cong, C Szepesvari, M Wang ce Optimization. NeurIPS 2022. \n",
            "368 :  73 :  Al-guided Directed Evolution optimizes Cas12a barcodes High-throughput screening measures barcode performance barcode sequence features TTTACCTG High-throughput Screening Enables Machine Learning Optimization of Cas12a Barcodes Cas12a target sites ML-optimized CRISPR-Cas'12a barcodes (DAISY: dual-acting inverted site array) Technology Neural network model predicts barcode capacity Select top predicted Cas12a barcode < Predicted ranking of barcode capacity -- Hughes, et al. Machine Learning Optimized Cas12a Barcoding Enables Recovery of Single-Cell Lineages and Transcriptional Profiles. Molecular Cell, 82, 3103-3118. \n",
            "369 :  74 :  ERIC AND WENDY CHMIDT CENTER Al-guided Directed Evolution optimizes Cas12a barcodes barcode sequence features ~~ ,——— (TTTACCTS .... AAA) — Piel ty 5 barcode capacity — Ikonaog ete AD Hgharoughot soning = meatus bare periomance — ; mas High-throughput Screening pacer liens Enables Machine Learning Optimization (44 a estza bores , ify (RAR) YS! Cest20 target stos / WC) << aH - - iu taaidvmntccve rected te sso prsceieag Teemolony ; -- Hughes, et al. Machine Learning Optimized Cas12a Barcoding Enables Recovery of Single-Cell Lineages and Transcriptional Profiles. Molecular Cell, 82, 3103-3118. \n",
            "370 :  75 :  ERIC AND WENDY SCHMIDT CENTER ERS at meoap institute Al-guided Directed Evolution optimizes Cas12a barcodes mM TACT A pene i h barcode capacity ‘AND WENDY. SCHMIDT CENTER = micro-homology features, Higharoughotsrening = meatus bare prima eo eos High-throughput Screening 1, Select top predicted / ee ae Sere ees VASA ( ‘of Casi2a Barcodes WOT A iH - si rectnology -- Hughes, et al. Machine Learning Optimized Cas12a Barcoding Enables Recovery of Single-Cell Lineages and Transcriptional Profiles. Molecular Cell, 82, 3103-3118. \n",
            "371 :  75 :  ERIC AND WENDY SCHMIDT CENTER ERS av mgoap institute Al-guided Directed Evolution optimizes Cas12a barcodes barcode sequence features - [—— (ETTACCTSS AAA) —+ eee barcode capacity = micro-homology features, Hgharoughptsreening 5 meatus baie priomance : tS High-throughput Screening vate Enables Machine Learning Optimization S544 eae wisp? extra / NOVA = - - un — -- Hughes, et al. Machine Learning Optimized Cas12a Barcoding Enables Recovery of Single-Cell Lineages and Transcriptional Profiles. Molecular Cell, 82, 3103-3118. \n",
            "372 :  75 :  ERIC AND WENDY SCHMIDT CENTER ERS av mgoap institute Al-guided Directed Evolution optimizes Cas12a barcodes barcode sequence features hs p——— (TTTACCTS ... AAA) — re peae barcode capacity = micro-homology features Higharoughotsrening = meatirou bare primar ¥f a 1 oe 7 Select top predicted High-throughput Screening ra es Enables Machine Learning Optimization ( (( of Cas12a Barcodes f WIVOTA =H - i\" — -- Hughes, et al. Machine Learning Optimized Cas12a Barcoding Enables Recovery of Single-Cell Lineages and Transcriptional Profiles. Molecular Cell, 82, 3103-3118. \n",
            "373 :  76 :  ERIC AND WENDY SCHMIDT CENTER SCHMIOT CENTER, Language model for decoding and ee. optimizing design of mRNA vaccines \\4 EBROAD \n",
            "374 :  76 :  ERIC AND WENDY SCHMIDT CENTER SCHMIDT CENTER | Language model for decoding and optimizing design of mRNA vaccines \n",
            "375 :  76 :  ERIC AND WENDY SCHMIDT CENTER Language model for decoding and optimizing design of mRNA vaccines \n",
            "380 :  76 :  ERIC AND WENDY SCHMIDT CENTER Language model for decoding and a optimizing design of mRNA vaccines \\ ea \n",
            "381 :  77 :  ERIC AND WENDY SCHMIDT CENTER Integrate Al into engineering PRINC E TON AlI2 Foster innovation AI Accelerated Accelerate breakthroughs Innovation Al for engineering design Al and language models for bioengineering Al for Fusion control Princeton Al Research Postdoc Fellow Openings!! KC Keller Center \n",
            "382 :  77 :  ERIC AND WENDY SCHMIDT CENTER Integrate Al into engineering Foster innovation Accelerate breakthroughs PRINCETON AI2 AI Accelerated Innovation Al for engineering design Al and language models for bioengineering Al for Fusion control Princeton Al Research Postdoc Fellow Openings!! \\( tet le —, \") \n",
            "383 :  77 :  ERIC AND WENDY SCHMIDT CENTER Integrate Al into engineering Foster innovation Accelerate breakthroughs PRINCETON AI2 AI Accelerated Innovation Al for engineering design Al and language models for bioengineering Al for Fusion control Princeton Al Research Postdoc Fellow Openings!! \\( tet) \n",
            "384 :  77 :  ERIC AND WENDY = SCHMIDT CENTER Integrate Al into engineering Foster innovation Accelerate breakthroughs PRINCETON AI2 Al for engineering design Al and language models for bioengineering Al for Fusion control Princeton Al Research Postdoc Fellow Openings!! KC tet) le ERIC AND WENDY SCHMIDT CENTER ror \n",
            "385 :  77 :  ERIC AND WENDY = SCHMIDT CENTER Integrate Al into engineering Foster innovation Accelerate breakthroughs PRINCETON AI2 Al for engineering design Al and language models for bioengineering Al for Fusion control Princeton Al Research Postdoc Fellow Openings!! \\( fete ERIC AND WENDY. SCHMIDT CENTER \n",
            "390 :  78 :  r{ \n",
            "391 :  79 :  +e \n",
            "392 :  80 :  N a fe ey = 6 \n",
            "395 :  81 :  stim ERIC AND WENDY CHMIDT CENTER Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation 2s) SeverelExrapelation | sate Dietrbtion Shit pas 5 | Citi Dearadation”” G54 Fidelity & Eis, Good Data Coverage: ; ove Distribution Shift Monotone Quality 5, Improvement \"Target Reward Value « \"Target Reward Value « Target Reward Value « \n",
            "396 :  81 :  ERIC AND WENDY CHMIDT CENTER Increasing target reward always leads to better samples? EE BROA * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation 2 Severe Extrapolation a2, targe Diatrbution Shift as s Quality Degradation” Gogg Fidelity Jul a Good Data Coverage: Monotone Quality Improvement Distribution Shift “Target Reward Value a “Target Reward Value a Target Reward Value « \n",
            "397 :  81 :  ERIC AND WENDY CHMIDT CENTER Increasing target reward always leads to better samples? == BROA SS Nstire * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation 2s, Severe Extrapolation: aa. ___Large Distribution Shift as 5 ! (Quay DegFAdRBGN” —Gooy Fidelity 21s Good Data Coverage: Fo Distribution Shift Monotone Quality 2 Improvement l Target Reward Value « “Target Reward Value a “Target Reward Value a \n",
            "398 :  81 :  ERIC AND WENDY CHMIDT CENTER Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation 2 Severe Extrapolation: pa enge Distribution hie | gous s Quilty Degradation” Gogg Fidelity Bul a Good Data Coverage: Monotone Quality Improvement Distribution Shift “Target Reward Value a” °° Farget Reward Value a Target Reward Value « \n",
            "399 :  81 :  ERIC AND WENDY R Increasing target reward always leads to better samples? F=B ESS * Over extrapolation can lead to compounding off-support error and low i data quality * Numerical example: ion Shite Generated Average Reward Off-support Deviation 2s|— [SeverelEstrapotawrey | Lrge Distribution Shift pas 97 Quality Degradation Good Fidelity 31s, Good Date Coverage: Monotone Quality Improvement Distribution Shift ‘Target Reward Value eS o8*Target Reward Valuea? —*° O° \"target Reward Value a \n",
            "400 :  82 :  ERIC AND WENDY CHMIDT CENTER Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation 2 Severe Extrapolation: a2, targe Diatrbution Shift as 5 i Quilty Degradation Gogg Fidelity dus 21s Good Data Coverage: Monotone Quality Improvement Distribution Shift “Target Reward Value a)” oFarget Reward Value a Target Reward Value a \n",
            "401 :  82 :  ERIC AND WENDY CHMIDT CENTER Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation Severe Extrapolation: Longe Distribution Shift ia ! Quality Degradation Good Fidelity & 20) i 315, Good Data Coverage: Monotone Quality Improvement . Distribution Shift “Target Reward Value a eFarget Reward Value a Target Reward Value « \n",
            "403 :  82 :  Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation re 05s: boo as Severe Extrapolation: | 5 oe aa! Large Distribution Shift | 32) : a0. § a §20 Sas = . Good Fidelity ¢ 20 g i 81s “tie Bie 2 | 8 31s Good Data Coverage: gt i g090 Small Distribution Shift Zos Monotone Quality | So25 % Improvement | S00 ! 020 10 3 oo 19-29 _-¥0 40S 00 50 00 50 Target Reward Value « 10, 20-3040 1220340 Target Reward Value a Target Reward Value a \n",
            "404 :  82 :  ERIC AND WENDY R ‘SCHMIDT CENT Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Generated Average Reward Off-support Deviation ° Quality Degradation Good Fidelity G20 z Bas oss 31s, Good Data Coverage: Monotone Quality Improvement Distribution Shift Target Reward Valu “Target Reward Value a” °° Target Reward Value a \n",
            "405 :  82 :  ERIC AND WENDY SCHMIDT CENTER Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation i} | Large Distribution Shift | as : me Q s Os Orth ood ray - ! as é i 1. i i 31s Good Data Coverage: ; Foa0 ‘Small Distribution Shift os Monotone Quality | 5 ‘Average Reward of Generated Samples Improvement | 179 9 a0 Te 33 eo \"Target Reward Value a \"Target Reward Value a 19709946 “Target Reward Value « \n",
            "408 :  83 :  ERIC AND WENDY SCHMIDT CENTER SSCHMIOT CENTER Problem Setup: Offline Optimization * Given a training data set, generate new x * Training data set Label unlabel label Diabei = {@i, yi = f* (ai) + es er\" — > & is observation noise > ftis (unknown) reward function \n",
            "409 :  84 :  Adding Guidance to Diffusion Models * Class-conditioned sample generation Noise Data _ & —-@©——_©—@ Zz axe = E XP +Viogpr FI | di+aW, Guidance Conditional Score Brownian \n",
            "410 :  84 :  ERIC AND WENDY i R Adding Guidance to Diffusion Models * Class-conditioned sample generation Noise Data re an Zt dt+dW, Conditional Score Brownian \n",
            "411 :  84 :  ERIC AND WENDY i R Adding Guidance to Diffusion Models * Class-conditioned sample generation Noise Data dt + dW, Conditional Score Brownian \n",
            "412 :  84 :  Adding Guidance to Diffusion Models * Class-conditioned sample generation Noise Data _o oe © fe is t Guidance r=[! wnececeeeeeee si Conditional Score Brownian \n",
            "413 :  85 :  o \n",
            "414 :  85 :  = ERIC AND WENDY CHMIDT CENTER Adding Guidance to Diffusion Models * Class-conditioned sample generation t © Conditional Score Brownian \n",
            "415 :  85 :  = ERIC AND WENDY SCHMIDT CENTER Adding Guidance to Diffusion Models * Class-conditioned sample generation ® Conditional Score Brownian \n",
            "416 :  85 :  = ERIC AND WENDY CHMIDT CENTER Adding Guidance to Diffusion Models * Class-conditioned sample generation Brownian © Conditional Score Brownian \n",
            "417 :  85 :  sq ERIC AND WENDY CHMIDT CENTER Adding Guidance to Diffusion Models * Class-conditioned sample generation wnian Guidance ox = [ar igi Ta eo Conditional Score Brownian \n",
            "418 :  85 :  ERIC AND WENDY CHMIDT CENTER Adding Guidance to Diffusion Models * Class-conditioned sample generation Noise XK ell ij dt+dW; Conditional Score Brownian \n",
            "426 :  86 :  ERIC AND WENDY CHMIDT CENTER Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation Severe Extrapolation: | Longe Distribution Shift g7) Quality Degradat Good Fidelity Eis, Good bat Coverage: ; Distribution Shift Monotone Quality a Improvement 7 : 7 5 . Target Reward Value « “Target Reward Value « \" Target Reward Value « \n",
            "427 :  86 :  ERIC AND WENDY CHMIDT CENTER Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality | * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation Severe Extrapolation: Large Distribution Shift a Quality Degradati Good Fidelity 21s, Good Data Coverage: Distribution Shift Monotone Quality Improvement “Target Reward Value a “Target Rewaed V. Target Reward Value « \n",
            "428 :  86 :  ERIC AND WENDY CHMIDT CENTER Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation Severe Extrapolation: Longe Distribution Shift Quality Degradati Good Fidelity 21s Good Data Coverage: Distribution Shift Monotone Quality Improvement “Target Reward Value a” O° Farget Reward Vi Target Reward Value « \n",
            "429 :  86 :  sien ERIC AND WENDY CHMIDT CENTER Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation Severe Extrapolation: Longe Distribution Shift g” Quality Degradation Good Fidelity 21s Good Data Coverage: Monotone Quality Improvement Distribution Shift Target Reward Value « “Target Reward Value a “Target Reward Value a \n",
            "430 :  86 :  sian ERIC AND WENDY CHMIDT CENTER Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation 2 Severe Extrapolation: Pe Large Disteibution Shift * eas € Quality Degradation Good Fidelity 31s Good Data Coverage: Monotone Quality Improvement Distribution Shift “Target Reward Ve Target Reward Value « “Target Reward Value a \n",
            "433 :  87 :  Witte ee \n",
            "438 :  88 :  Mengdi Wang Associate Professor of Electrical and Computer Engineering, Princeton University \n",
            "449 :  89 :  Copyright © 2024 Broad Institute of MIT and Harvard Please contact communications@broadinstitute.org for permission to use, copy, or modify any portion of this video. \n",
            "\n",
            "The text is extracted from slides images and each slide is\n",
            " separated by a |. Your task is to\n",
            " extract all the main points in the text in organized bullet and\n",
            " sub-bullet points. Don't miss any points in the text, don't change order of the text, don't change the words.\n",
            "\n",
            "Text:\n",
            " ```i J Pr , AL ls| P< Caroline Uhler S Director, Eric & Wendy Schmidt Center| .| ERIC AND WENDY SCHMIDT CENTER Guiding Diffusion Models Towards 3ROAD Generative Optimization Ley Mengdi Wang Princeton Al2 Initiative Electrical and Computer Engineering, Center for Statistics & Machine Learning Princeton University| ERIC AND WENDY SCHMIDT CENTER Emergence of Generative Al 29 Coie tr ChatGPT BERT Stable Diffusion Transformer Deep Generative Al 1998 2012 2015 2018 2021 = 2022 2023 2024 ~- Thanks to blogs by Rockwell Anyoha , Toloka Team and Rick Merritt| ERIC AND WENDY SCHMIDT CENTER Family of Deep Generative Al 7 Fake: VAE (Kingma & Welling, 2013) GAN (Goodfellow et al., 2014) Generative Al omer on - gS| ERIC AND WENDY SCHMIDT CENTER Diffusion model Sora by OpenAl RFDiffusion by UW| stig ERIC AND WENDY CHMIDT CENTER A Revolution - Diffusion Model * Sequential transformation Noise Data (Sohl-Dickstein et al., 2015) > 890M parameters (Song and Ermon, 2019) (Stable Diffusion) (Ho et al., 2020)| ERIC AND WENDY CHMIDT CENTER A Revolution - Diffusion Model * Sequential transformation Noise Data a; e 4 Mt ‘ | tH HH ie ry | sem Backbone of Stable Diffusion, DALL-E, Sora etc. ey ; (Sohl-Dickstein et al., 2015) > 890M parameters (Song and Ermon, 2019) (Stable Diffusion) (Ho et al., 2020)| ERIC AND WENDY SCHMIDT CENTER Foundations of Diffusion Models| SCHMIDT CENTER ERIC AND WENDY y Diffusion Model Generates Samples * Generate samples from noise Noise Data =—z -- Credit: online| ERIC AND WENDY SCHMIDT CENTER RIC AND WENDY SCHMIDT CENTER Diffusion for protein structure generation Forward (noising) process ———S— re r ome No. 1) ¥ Single ¥ = step Gaussian ~ Protein < noise co Rte, —> cg x structure se Z e. ee a We Pe LA Xx, xX. X > Reverse (generative) process -- credit: RFDiffusion from Baker's group, 2023| ERIC AND WENDY R AND WENDY SCHMIDT CENTER Diffusion for protein structure generation J Forward (noising) process _——— No. ¥ Single ¥ step * als Protein Rex —> 1 0D) ¥ structure ' ek, We x, LA x, xX, AX, — ae Reverse (generative) process Gaussian ~ noise -- credit: RFDiffusion from Baker's group, 2023| ERIC AND WENDY SCHMIDT CENTER Diffusion for protein structure generation Forward (noising) process <—_ ) No. ¥ Single ¥ r step Protein t a — ee AS structure x, LA Xx, Xen LA X en Reverse (generative) process Gaussian ~» % noise -- credit: RFDiffusion from Baker's group, 2023| ERIC AND WENDY R Forward Process - Noise Corruption + Noise corruption process Add Gaussian noise Data Approx. Noise ' M @—@ © é & a 1 dX; = Ta Xedt +dW; * The noise corruption Lebo bb Data Approx. Noise| Backward Process - Sample Generation * Time reversal in distribution Noise Vlog pr-(Xi-) Data Q—@©—_*— ©@—® a c a ¢ The math (Anderson, 1982; Haussmann and Pardoux, 1986) Teer, 1 Forward dX, = ~gXede +dw, Backward dxf = Face +iV log pr—1(X/)) dt + dW; Score Function Brownian| Backward Process - Sample Generation * Time reversal in distribution Noise V log pr-i(X7\") Data ©—@—_*—-©—® a it * The math (Anderson, 1982; Haussmann and Pardoux, 1986) Forward dX; = —5Xat +dW; Backward dxf = [pao iV log pr— a7 dt+dW, tee Score Function Brownian a| ERIC AND WENDY SCHMIDT CENTER HMIDT CENTER Forward and Backward Coupling _ . Gaussian Noise Spiciceacns Approx. Noise| Forward and Backward Coupling Gaussian Noise Data eck eae ‘. Approx. Noise A “. Forward Se . ‘¢ ¢ > a ai Ne x! Density 7\" °s, 4 -<ften Noise nS Data | ~| san ERIC AND WENDY HMIDT CENTER Forward and Backward Coupling Gaussian Noise . Data ence, Approx. Noise © on Z H.. me OSty 7 Noise Data O—-F——\"o—-8 Viogpr-t mes ¥| Forward and Backward Coupling Gaussian Noise Data eae eae So = s Density co Forward © an Data o_o a Approx. Noise| ERIC AND WENDY CHMIDT CENTER Score Function Estimation * Conceptual least-square loss [Be lV 0s pe.) — (018) a * Early-stopping loss [ Ez, {|| V log pe(ae) — s(ae, t)|I3] at * Equivalent loss (Hyvarinen and Dann, 2005; Vincent 2011) Added Gaussian Noise| ERIC AND WENDY SCHMIDT CENTER Can we trust diffusion models? Forward (noising) process Now) ¥ Single ¥ step Protein Se ANE structure x LAX xX. LAX, —————— Reverse (generative) process Gaussian ~ noise Q1: Can diffusion models learn the underlying data distribution? Q2: Can we control the generation of diffusion models towards specific objectives? How efficiently? -- credit: RFDiffusion from Baker's group, 2023| ERIC AND WENDY SCHMIDT CENTER ERS av mgoav institute @g ERIC AND WENDY Practical Data Is Structured and Low-Dim Nom k=3 wae k=5 we k=10 om k=20 0 FS a0 @ i] : @ @ . a i Tea pot in 1D Effective dim of image datasets -- Figure credit: (Weinberger & Saul, 2006; P. Pope et al., 2021)| Can Diffusion Model Learn Data's Intrinsic Structure? * The (backward) generation process Score Function” Brownian * Geometric data adds another layer of difficulty Manifold (degenerate) t| Intrir Brownian| ERIC AND WENDY CHMIDT CENTER Can Diffusion Model Learn Data's Intrinsic Structure? * The (backward) generation process Score Famtion’ Brownian * Geometric data adds another layer of difficulty Manifold egenerate)| ERIC AND WENDY SCHMIDT CENTER AMIDT CENTER Let's work with Low-dimensional Data | - ae w(x) =Az with z~ P,. = A € R*¢ js unknown with orthonormal columns. = z € R¢ isa latent variable. = wis a known invertible transformation, e.g., feature map of RKHS and Fourier transform. (For simplicity, we take x = J in the sequel.) mu D - ambient dimension; d - intrinsic dimension.| ERIC AND WENDY CHMIDT CENTER Let's also simplify the score network's architecture Shortcut pat Seqnetatan map Kl hose —h-\\(t) A(t) Ré 8 f 8v,0(x, t) 1 t Encoder fo Decoder [eh Hed corsa aw ! { conto La 0 ea tsi bee oo| ERIC AND WENDY CHMIDT CENTER Let's also simplify the score network's architecture Shortcut | out segnentaton map h(t) A(t) x RP R! RP 8v,0(x, t) Encoder fo Decoder| Key Insight: Decomposition * Score decomposition V log p:(x) =! FAV log pi(A\"2),— = (Ip = AA’) a! eases ae ; On-subspace Orthogonal Lt, Lty 3 Generated 1 Sample mY 77 NIrlection Subspace Lt| ste ERIC AND WENDY CHMIDT CENTER From Subspace to Manifold Diffusion Subspace Manifold Th Te, Te ty ; ° 1 1 ++ Pee) ~~) Prefection ts Ts Score = On-subspace + logon Score = On-manifold + Orth t + Curvature-dependent ross-term| gti ERIC AND WENDY CHMIDT CENTER RIC AND WENDY ff SCHMIDT CENTER From Subspace to Manifold Diffusion Subspace Manifold Pe Lt, Lt, y ¢ ° _ | . _ <<) Prafection Local 2 ty projection Score = On-subspace + logon Score = On-manifold + Orthox + Curvature-dependent| stim ERIC AND WENDY CHMIDT CENTER From Subspace to Manifold Diffusion v SCHMIDT CENTER Subspace Manifold TH te, t% 8 : ; 1 + rae ~~) Prefection . oca * tty projection Score = On-subspace + Orthogon Score = On-manifold + Orthogo + Curvature-dependent ross-term| ERIC AND WENDY CHMIDT CENTER Diffusion model is an efficient density estimator Theorem ¥ (Distribution). Converge to the data distribution at the rate oO (n- 7a ) Y (Fidelity). Deviation to the subspace is bounded by 9 (n=), Y Efficient in modeling data distributions v Adaptive to manifold structures ng. “s Approximati timation an Dimensior ICML 2023| ERIC AND WENDY SCHMIDT CENTER 5) Senior ceNTeR | Can we trust diffusion models? Forward (noising) process Now) ¥ Single ¥ step Protein sai: < ee Ne We structure x LAX xX. LAX, Reverse (generative) process Gaussian ~ noise Q1: Can diffusion models learn the underlying data distribution? Q2: Can we control the generation of diffusion models towards specific objectives? How efficiently? -- credit: RFDiffusion from Baker's group, 2023| Guiding diffusion model to generate new data of specific objective * RFDiffusion can generate binder design from a user-specified binding tarnnt Binding target Binder design| stis ERIC AND WENDY CHMIDT CENTER Adding Guidance to Diffusion Models * Class-conditioned sample generation Noise © < it Conditional Score Brownian| Noise Brownian| sen ERIC AND WENDY CHMIDT CENTER Adding Guidance to Diffusion Models * Class-conditioned sample generation EE BRO Noise Data — @—© —_@—® | = do = i dxf = Conditional Score Brownian Classifier-free guidance (Ho & Salimans, 2022) Classifier guidance (Dhariwal & Nichol, 2021)| ERIC AND WENDY SCHMIDT CENTER ERIC AND WENDY. ‘SCHMIDT CENTER Rethinking optimization as generative Al max y = f(x) z~ P(X|Y = fmaz) High-D, nonconvex Can we solve harder problems? Structure hard to model Why?| ERIC AND WENDY SCHMIDT CENTER Problem Setup: Offline Optimization * Given a training data set, generate new x * Training data set EE BROAD Label = f(a) + ea} = Diabel = (es Y= > & is observation noise > pris (unknown) reward function Examples:| ERIC AND WENDY SCHMIDT CENTER SCHMIDT CENTER Problem Setup: Offline Optimization =. = * Given a training data set, generate new x * Training data set _— Label BROAD Dinadel = {x; peace. Diaber = {xi, yi = f* (ai) + ce }e\" > & is observation noise > pris (unknown) reward function Examples: Qa large collection of unlabeled protein structures; only a few has measured properties.| ERIC AND WENDY SCHMIDT CENTER Problem Setup: Offline Optimization * Given a training data set, generate new x * Training data set Label ROAD Duniabel = {25 }524\"\" Diaber = {@i, yi = f* (ai) + ce }Ee\" > & is observation noise > pris (unknown) reward function Examples: Qa large collection of unlabeled protein structures; only a few has measured properties.| ERIC AND WENDY = SCHMIDT CENTER Problem Setup: Offline Optimization * Given a training data set, generate new x * Training data set Label Bos ie at : z label Diaber = {xi, yi = f* (wi) + ce }eEe\" > & is observation noise > pris (unknown) reward function Examples: Qa large collection of unlabeled protein structures; only a few has measured properties.| Problem Setup: Offline Optimization * Given a training data set, generate new x * Training data set Label Diabel = {2i, yi = f* (ai) + a ee 65 P P > & iS observation noise f P Off-policy bandit problem > fils (unknown) reward function (Jin et al., 2021; Nguyen-Tang et al., 2021) Examples: Q a large collection of unlabeled protein structures; only a few has measured properties. Q RNA-seq atlas, with only few annotations| ERIC AND WENDY CHMIDT CENTER Problem Setup: Offline Optimization * Given a training data set, generate new * Training data set Dunlabel = {25 }525\"\" Diabei = {@i, yi = f* (ai) + See\" > & is observation noise > pris (unknown) reward function Examples: Qa large collection of unlabeled protein structures; only a few has measured properties. Q RNA-seq atlas, with only few annotations (Jin et al., 2021; Nguye nTang et al, 2021) SCHMIDT CENTER BROAD yd| We propose a meta-algorithm: Reward- conditioned diffusion model Label Pseudo-label = Reward Model an Step 1: Reward Learning Step 2: Pseudo Labeling Reward-directed conditional diffusion: Provable distribution estimation and reward improvement. NeurIPS, 2022 34| We propose a meta-algorithm: Reward- conditioned diffusion model Label Pseudo-label Reward Model oo = Step 1: Reward Learning Step 2: Pseudo Labeling Conditional Score O P(- | reward) Conditional Diffusion Step 3: Conditional Diffusion Training Reward-directed conditional diffusion: Provable distribution estimation and reward improvement. NeurlPS, 2022 34| Pseudo-label esl 2: Pseudo Labeling Step 3 Conditional Diffusion Ty,| We propose a meta-algorithm: Reward- conditioned diffusion model Label Pseudo-label > Reward Model a Step 1: Reward Learning Step 2: Pseudo Labeling Conditional Score reward = a <a) Conditional Diffusion Step 3: Conditional Diffusion Training Step 4: Guided Generation Reward-directed conditional diffusion: Provable distribution estimation and reward improvement. NeurIPS, 2022 34| ERIC AND WENDY R We propose a meta-algorithm: Reward- conditioned diffusion model ae Label - en Pseudo-label — eward Model =. ana — BROAD) Step 1: Reward Learning Step 2: Pseudo Labeling rewarc ard) ——— Diffusion Model | as : Guided Generation le distribution estimation and reward improvement. NeurlPS, 2022 Step 3: Conditional Diffusion Tr onditional diffusion: P ard-direc| ERIC AND WENDY R We propose a meta-algorithm: Reward- conditioned diffusion model Label — Reward Model = Step 1: Reward Learning ‘onditional diffusi Pseudo-label — Step 2: Pseudo Labeling Diffusion Model —| ion: Prova le distribution estima : Guided Generation tion and reward improvement. NeurIPS, 2022 scrim CENT RI ‘BROAD| ERIC AND WENDY R We propose a meta-algorithm: Reward- conditioned diffusion model Label Reward Model =. Step 1: Reward Learning Conditional Diffusion Step 3: Conditional Diffusion Tr nditional diffu Pseudo-label — Step 2: Pseudo Labeling ————— Diffusion Model —_ i ae : Guided Generation ment. NeurlPS, 2022 le distribution estimation and reward imp!| ERIC AND WENDY R Reward-condition diffusion for generative optimization Question: How far are we from the target objective value? How good are new samples? Target Training Diabet raining| ERIC AND WENDY SCHMIDT CENTER Optimization theory (linear model) Theorem v¥ New samples are high-fidenty. The sub-optimality satisfies Sub0pt(a) = O ( Trace (8572.) Jaf ee +min{a, d} - ee ee ) J ‘abel Nunlabel where 8, =(X7X +AJ)/napafor x the data matrix, \\>0, and 5, is the covariance matrix of P,(- | reward = a). H. Yuan, K. Huang, C. Ni, M. C, M. Wang. “Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement”, NeurIPS 2023| ERIC AND WENDY CHMIDT CENTER Optimization theory (linear model) Theorem v New samples are high-fidenty. The sub-optimality satisfies dlog(n 4 ? Sub0pt(a Trace ( $315.) dlog (Mader) label) +H . Mabel : ' . where &, = (X7X +AJ)/napafor Xx the data matrix, \\>0, and 5, is the covariance matrix of P,(- | reward = a).| ERIC AND WENDY R Optimization theory (linear model) agp Theorem NN v¥ New samples are high-fidenty. The sub-optimality satisfies f =i ROAD dlog(mavei) Ms ) ; Nabel : ' x ; Sub0pt(a) = O | Trace (83724) where 8, =(X7X +AJ)/napafor x the data matrix, \\>0, and =, is the covariance matrix of P,(- | reward = a). I icy k i ing with known representations (Jin et al., 2021; Nguyen-Tang et al., 2021)| ERIC AND WENDY R Advantages of Generative Optimization VY Meta algorithm provably generates samples of high reward and fidelity, in nonparametric settings. a oe —aéan subopt(a) = 6 (x(a) ml + (a) mae J to learn structures from unlabeled data new samples maintain data’s intrinsic structures v provable optimality H. Yuan, K. Huang, C. Ni, M. C rovable Distribution E d Improvement”, NeurIPS 2023| ERIC AND WENDY R Advantages of Generative Optimization VY Meta algorithm provably generates samples of high reward and fidelity, in nonparametric settings. = a —aéan subope(a) =O (ria) ml + aa) Mase J to learn structures from unlabeled data new samples maintain data’s intrinsic structures v provable optimality| ERIC AND WENDY i R Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: ion Shift Generated Average Reward Off-support Deviation ! 25 Severe Extrapolation: | Large Distribution Sift cas =? Nv iia Degradation” Geog Fidelity $7 ! g i. gos 21s Good Data Coverage: | Distribution Shift Monotone Quality Leal ( Improvement ig Xo x9 i; 50 oo 10, 20 x0 ao 19, 20 xo «0 ‘Target Reward Value « i ‘Target Reward Value « ‘Target Reward Value «| ERIC AND WENDY R Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation g Qiiality Degradation” —Goog Fidelity $20 ee 3s soos 21s Good Data Coverage: Fo Distribution Shift Monotone Quality a | Improvement a <3 we 3g a5 oe 3g ea 80 ‘Target’ Reward Value a” ararpst Rowen Vane Gn Trae? Reward Vane| ERIC AND WENDY 235: SCHMIDT CENTER Reward improves as target value increases Watch for distribution shift!| Numerical experiment with optimal control: Generate state trajectories conditioned on high value Hopper Control eof Match SOTA MOReL anus Avg. reward FSF pe pips? kas 4 , Tart reward Surprise: Diffusion optimizer is comparable to best RL algorithm in some control tasks| Numerical experiment with optimal control: Generate state trajectories conditioned on high value Hopper Control tof Match SOTA MORoL user » ep ° ” - = Avg. reward pp Target reward Surprise: Diffusion optimizer is comparable to best RL algorithm in some control tasks| ERIC AND WENDY SCHMIDT CENTER Numerical experiment with optimal control: Generate state trajectories conditioned on high value Target reward Hopper Control EEBROAD Avg. reward Diffusion optimizer is comparable to best RL algorithm in some control tasks| ERIC AND WENDY CHMIDT CENTER A new modality: Generative Optimization Guidance Conditional Diffusion Pretraining + Online ¥ High-Dim structured data Y Life-long adaptation v Sample efficiency ote| ERIC AND WENDY SCHMIDT CENTER An analogy to guided diffusion: sided directed evolution for J me g Directed % Evolution i & parent > @mplificatio®| Thompson Sampling-guided directed evolution provably explores sequence space mutation recombination Targeted sites ¥ y Directed Mutation Crossover Selection ‘Original seq. fi 1 ro ) | =>! S; Mutated seq. Possible’ehitdren -- Hi Yuan, C Ni, H Wang, X Zhang, L Cong, C Szepesvari, M Wang. Bandit Theory and Thompson Sampling-Guided Directed Evolution for Sequence Optimization. NeurIPS 2022.| ERIC AND WENDY SCHMIDT CENTER Thompson Sampling-guided directed evolution provably explores sequence space mutation recombination || directed mutation crossover selection Hi Yuan, C Ni, H Wang, X Zhang, L Bandit Theory and Thompson Sampling-Guided Directed Evolution for Sequence ( ng, C Szepesvari, M Wang imization. NeurIPS 2022| Thompson Sampling-guided directed evolution provably explores sequence space ERIC AND WENDY SCHMIDT CENTER Si-1 = Crossover Selection rm & —. Thompson_Sampling ¢ ——— Posterior Model |Sachupdare| Dataset mutation recombination } Hi Yuan, C Ni, H Wang, X Zhang, L Cong, C Szepesvari, M Wang Bandit Theory and Thompson Sampling-Guided Directed Evolution for Sequence Optimization. NeurIPS 2022| ERIC AND WENDY SCHMIDT CENTER Thompson Sampling-guided directed evolution provably explores sequence space mutation recombination || oirected mutation crossover. selection | Thompson_Sampling 2 ected Munaton ——— Posterior 1 crenore Stein +——]_ Moder | Sacnopaare] Dataset Hi Yuan, C Ni, H Wang, X Zha Bandit Theory and Thompson Sampling-Guided Directed Evolution for Sec g, L Cong, C Szepesvari, M Wang ce Optimization. NeurIPS 2022.| Al-guided Directed Evolution optimizes Cas12a barcodes High-throughput screening measures barcode performance barcode sequence features TTTACCTG High-throughput Screening Enables Machine Learning Optimization of Cas12a Barcodes Cas12a target sites ML-optimized CRISPR-Cas'12a barcodes (DAISY: dual-acting inverted site array) Technology Neural network model predicts barcode capacity Select top predicted Cas12a barcode < Predicted ranking of barcode capacity -- Hughes, et al. Machine Learning Optimized Cas12a Barcoding Enables Recovery of Single-Cell Lineages and Transcriptional Profiles. Molecular Cell, 82, 3103-3118.| ERIC AND WENDY CHMIDT CENTER Al-guided Directed Evolution optimizes Cas12a barcodes barcode sequence features ~~ ,——— (TTTACCTS .... AAA) — Piel ty 5 barcode capacity — Ikonaog ete AD Hgharoughot soning = meatus bare periomance — ; mas High-throughput Screening pacer liens Enables Machine Learning Optimization (44 a estza bores , ify (RAR) YS! Cest20 target stos / WC) << aH - - iu taaidvmntccve rected te sso prsceieag Teemolony ; -- Hughes, et al. Machine Learning Optimized Cas12a Barcoding Enables Recovery of Single-Cell Lineages and Transcriptional Profiles. Molecular Cell, 82, 3103-3118.| ERIC AND WENDY SCHMIDT CENTER ERS av mgoap institute Al-guided Directed Evolution optimizes Cas12a barcodes barcode sequence features hs p——— (TTTACCTS ... AAA) — re peae barcode capacity = micro-homology features Higharoughotsrening = meatirou bare primar ¥f a 1 oe 7 Select top predicted High-throughput Screening ra es Enables Machine Learning Optimization ( (( of Cas12a Barcodes f WIVOTA =H - i\" — -- Hughes, et al. Machine Learning Optimized Cas12a Barcoding Enables Recovery of Single-Cell Lineages and Transcriptional Profiles. Molecular Cell, 82, 3103-3118.| ERIC AND WENDY SCHMIDT CENTER Language model for decoding and a optimizing design of mRNA vaccines \\ ea| ERIC AND WENDY = SCHMIDT CENTER Integrate Al into engineering Foster innovation Accelerate breakthroughs PRINCETON AI2 Al for engineering design Al and language models for bioengineering Al for Fusion control Princeton Al Research Postdoc Fellow Openings!! \\( fete ERIC AND WENDY. SCHMIDT CENTER| r{| +e| N a fe ey = 6| ERIC AND WENDY R Increasing target reward always leads to better samples? F=B ESS * Over extrapolation can lead to compounding off-support error and low i data quality * Numerical example: ion Shite Generated Average Reward Off-support Deviation 2s|— [SeverelEstrapotawrey | Lrge Distribution Shift pas 97 Quality Degradation Good Fidelity 31s, Good Date Coverage: Monotone Quality Improvement Distribution Shift ‘Target Reward Value eS o8*Target Reward Valuea? —*° O° \"target Reward Value a| ERIC AND WENDY SCHMIDT CENTER Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation i} | Large Distribution Shift | as : me Q s Os Orth ood ray - ! as é i 1. i i 31s Good Data Coverage: ; Foa0 ‘Small Distribution Shift os Monotone Quality | 5 ‘Average Reward of Generated Samples Improvement | 179 9 a0 Te 33 eo \"Target Reward Value a \"Target Reward Value a 19709946 “Target Reward Value «| ERIC AND WENDY SCHMIDT CENTER SSCHMIOT CENTER Problem Setup: Offline Optimization * Given a training data set, generate new x * Training data set Label unlabel label Diabei = {@i, yi = f* (ai) + es er\" — > & is observation noise > ftis (unknown) reward function| Adding Guidance to Diffusion Models * Class-conditioned sample generation Noise Data _o oe © fe is t Guidance r=[! wnececeeeeeee si Conditional Score Brownian| ERIC AND WENDY CHMIDT CENTER Adding Guidance to Diffusion Models * Class-conditioned sample generation Noise XK ell ij dt+dW; Conditional Score Brownian| sian ERIC AND WENDY CHMIDT CENTER Increasing target reward always leads to better samples? * Over extrapolation can lead to compounding off-support error and low data quality * Numerical example: Distribution Shift Generated Average Reward Off-support Deviation 2 Severe Extrapolation: Pe Large Disteibution Shift * eas € Quality Degradation Good Fidelity 31s Good Data Coverage: Monotone Quality Improvement Distribution Shift “Target Reward Ve Target Reward Value « “Target Reward Value a| Witte ee| Mengdi Wang Associate Professor of Electrical and Computer Engineering, Princeton University| Copyright © 2024 Broad Institute of MIT and Harvard Please contact communications@broadinstitute.org for permission to use, copy, or modify any portion of this video.```\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_body = \"\"\"\n",
        "The text is extracted from slides images and each slide is\n",
        " separated by a |. Your task is to\n",
        " extract all the papers referenced in these slides. Don't miss any points in the text, don't change order of the text, don't change the words.\n",
        "\n",
        "Text:\n",
        "\"\"\"\n",
        "\n",
        "response = yt_video_to_bullet(video_url, prompt_body)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "Llz0AamYHnWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # YouTube video URL with long text (Conference)\n",
        "# video_url = 'https://www.youtube.com/watch?v=hE135guhTQo'\n",
        "\n",
        "# prompt_body = \"\"\"\n",
        "# The text is extracted from slides images and each slide is\n",
        "#  separated by a |. Your task is to extract all the main points in the following\n",
        "#  text in organized bullet points.\n",
        "#  Don't miss any points in the text, don't change order of the text,\n",
        "#  don't change the words.\n",
        "\n",
        "# Text:\n",
        "# \"\"\"\n",
        "\n",
        "# extracted_text = yt_video_to_text(video_url)\n",
        "\n",
        "# response1 = text_to_bullets(extracted_text[:35], prompt_body)\n",
        "# response2 = text_to_bullets(extracted_text[35:], prompt_body)\n",
        "# prompt = f\"\"\"\n",
        "# Please merge these two bullet point texts.\n",
        "# Don't miss any points in the text, don't change order of the text,\n",
        "#  don't change the words.\n",
        "\n",
        "# Summary1: ```{response1}```\n",
        "# Summary2: ```{response2}```\n",
        "# \"\"\"\n",
        "\n",
        "# final_response = get_completion(prompt)\n",
        "# print(final_response)"
      ],
      "metadata": {
        "id": "ChBf-bmUDE3E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}